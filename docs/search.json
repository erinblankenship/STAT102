[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 102 Notes",
    "section": "",
    "text": "Course Goals for STAT 102\nSTAT 102 is an introduction to formal statistical inference. We will carry out inference using both simulation-based approaches and classical, theory-based methods. By the end of the course, you will:\n\nRead an example where the research question is explicitly stated, and then translate what’s stated into a statistical statement involving parameters or other simple distributional characteristics.\nIdentify whether the ideal data collection strategy would involve random assignment, random sampling, or both and explain why.\nWork with an example where the research question is explicitly stated, along with an existing data set, and propose and carry out an appropriate analysis to answer the research question.\nExplain the terms/components of a given statistical model, and connect those terms to the research question at hand.\nCheck basic assumptions of various (simple) analysis methods and justify the use of the method.\nApply existing functions and point-and-click software for implementing basic data analyses.\nUse tactile simulation to carry out a simple resampling procedure.\nIdentify the steps and perform the calculations required for routine statistical procedures to address a given problem.\nCalculate simple analyses (t-test, chi-squared test for proportions) by hand, to verify the validity of the computational algorithm.\nRecognize when computational results do not make sense in the context of the problem.",
    "crumbs": [
      "Course Goals for STAT 102"
    ]
  },
  {
    "objectID": "Section 1 Philosophy.html",
    "href": "Section 1 Philosophy.html",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "",
    "text": "1.1 Randomization Tests (Chapter 11)\nIn STAT 101, you focused on Exploratory Data Analysis. Exploratory data analysis aims to investigate the characteristics of a data set through visualizations and numerical summaries. Visualizations may include:\nNumerical summaries used to explore a data set may include:\nMore often than not, the data were collected to answer a research question about a larger population for which the data collected are a (hopefully) representative sample. This notion of drawing conclusions beyond the data collected is at the heart of statistical inference.\nExample: Bred in the Bone\nTake away:\nIn exploratory data analysis, the visualizations and numerical summaries you choose are driven by the type of data at hand. This is true for statistical inference as well. The type of data will drive the appropriate inference techniques. However, the goal of the research study will also impact the selected method, as will the underlying assumptions of the technique (we’ll talk a lot more about this). That said, there are some overarching approaches to quantifying variability, and thus drawing conclusions beyond the data set at hand.\nApproaches to quantifying variability\nWe’ll start the semester by talking about these three approaches fairly generally. For (most of) the rest of the semester, we’ll see how these approaches fit with different types of data.\nThe goal of hypothesis tests is to use an observed data set to answer a yes/no question about a characteristic of a larger population from which the observed data set was drawn. For example, is swimming with dolphins therapeutic for patients with clinical depression? That is, we want to assess whether or not the explanatory variable causes changes in the response variable.\nTo answer this question, Antonioli and Reveley (2005) recruited 30 subjects with a clinical diagnosis of mild to moderate depression. The subjects were required to stop all other treatments (therapy and/or pharmaceuticals) 4 weeks prior the experiment, and the 30 subjects were all taken to an island off the coast of Honduras. The subjects were randomly assigned to one of two groups. Both groups spent one hour swimming and snorkeling each day, but one group did so in the presence of dolphins and the other group did not. At the end of two weeks, each subject’s level of depressions was evaluated, and whether or not the subjects had a substantial improvement in their depression was recorded.\nExplanatory variable:\nResponse variable:\nIs this an observational study or an experiment? What does that imply about inference?\nThe question we will answer is whether the resulting data provide convincing evidence that subjects who swam with dolphins were more likely to see depression improvement than subjects who swam without dolphins.\nIf there really is no impact of swimming with dolphins, what does this imply about the explanatory and response variables?\nIf swimming with dolphins does improve depression, what does this imply about the explanatory and response variables?\nThis leads to two competing claims:\nIf the null hypothesis is true, how would this manifest in the observed data?\nIf the alternative hypothesis is true, how would this manifest in the observed data?\nWe will choose between the competing claims by assessing whether the data conflict so much with H\\(_0\\) that the null hypothesis cannot be considered reasonable. If this happens, we’ll reject the notion of H\\(_0\\) and conclude that H\\(_a\\) must be true.\nUp to now, we haven’t seen the data! Here’s a summary:\nWe can see that\nSo,\nThe question remains…is this enough different from what we would expect under the null hypothesis to conclude that swimming with dolphins does make a difference in depression?\nSo far, nothing we’ve laid out is unique to a randomization test. Where does randomization come in?\nLet’s visualize these observations as a set of cards. Each card denotes a subject in the study. The color indicates the response: red for substantial improvement and black for no substantial improvement.\nAny difference we see in the simulation is due to chance–the cards were randomly dealt into the dolphin/control groups.\nIt’s not realistic to keep shuffling and dealing by hand…we need to turn to technology to do the randomization for us: Applet\nWe can do this over and over again to build up a null distribution. This distribution shows how we expect the variability to behave under the null hypothesis:\nWhat do you notice about this null distribution?\nHow rare is it to see our observed statistic 0.467 in this distribution? What does this imply?\nSo, we’ve just carried out a statistical inference technique! We might be wrong in our conclusions (more on this in Chapter 14), but we’ve made the best decision we could with the data available.\nIn summary:\nRandomization Test Procedure:\nNow let’s go to R!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section 1 Philosophy.html#randomization-tests-chapter-11",
    "href": "Section 1 Philosophy.html#randomization-tests-chapter-11",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "",
    "text": "Null hypothesis: H\\(_0\\)\n\n\n\nAlternative hypothesis: H\\(_a\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDolphin Therapy\nControl Group\nTotal\n\n\n\n\nShowed Improvement\n\n\n\n\n\nNo Improvement\n\n\n\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne Shuffle\n\n\n\n\n\n\n100 Shuffles\n\n\n\n\n\n\n\n\n\n\nFrame the research question in terms of hypotheses\nCollect data from an observational study or experiment\nModel randomness that would occur if H\\(_0\\) is true\nAnalyze the data by comparing the observed data to the simulated distribution\nConclusion",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section 1 Philosophy.html#bootstrap-methods-chapter-12",
    "href": "Section 1 Philosophy.html#bootstrap-methods-chapter-12",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "1.2 Bootstrap Methods (Chapter 12)",
    "text": "1.2 Bootstrap Methods (Chapter 12)\nBootstrap methods are a relatively new statistical technique (proposed in 1979 by Efron), but they are based on a very simple idea. The goal is to characterize the variability of the statistic across many samples. One way we could do this is take lots and lots of samples from the population, and get a picture of how much variance there is among the samples. This is almost always impossible. So, rather than resample from the population, we could try resampling from the sample. This is the basic idea behind the bootstrap.\nBootstrapping is used in many different applications. For this general introduction to the approach, we’re going to consider a confidence interval for a proportion.\nA confidence interval is\n\n\n\n\n\nNote the goal of the confidence interval is different from the goal of a hypothesis test!\n\n\n\n\nHowever, like with hypothesis tests, we need to understand the variability inherent to the statistic. To figure out how wide the range of plausible values should be, we need to know how a statistic varies from sample to sample in the population.\nFor example, let’s think back to the Baby scenario and suppose our goal is to estimate the population parameter\n\n\n\n\nThe researchers collected one sample of 16 babies, and found that 14 picked the good guy. This is our observed data. What do you think would happen if we took a sample of 16 different babies? And then a different sample of 16 babies?\n\n\n\n\nIdea of the bootstrap:\n\n\n\n\n\n\n\nInfinite populations are pretty tough to work with, though. However, we can produce an equivalent bootstrap distribution by\n\n\n\n\nSo, we’ll repeatedly draw bootstrap samples of size 16 (why 16?) and calculate the proportion of successes in each bootstrap samples. After we do this many many times, we’ll have an idea of a range of plausible values for the population parameter. We’ll set the confidence level by opting for a wider or a narrower interval, based on how certain we need to be in the results.\n\n\n\n\nBootstrap Process\n\nFrame the research question in terms of a parameter to estimate\nCollect data using an observational study or an experiment\nModel the randomness by using the observed data as a proxy for the population\nCreate the interval (in future chapters we’ll see there are multiple ways to do this)\nConclusion\n\nLet’s go to R!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section 1 Philosophy.html#inference-with-mathematical-models-chapter-13",
    "href": "Section 1 Philosophy.html#inference-with-mathematical-models-chapter-13",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "1.3 Inference with Mathematical Models (Chapter 13)",
    "text": "1.3 Inference with Mathematical Models (Chapter 13)\nSo far, we’ve seen computational methods like randomization and bootstrapping to characterize the variability of a statistic. The use of computational methods is relatively recent, due to the increase in computing power. In pre-computing days, re-sampling and randomization was very difficult. As a result, mathematical approximations were used and are still pervasive. If you took AP Statistics or a different intro statistics course, you employed mathematical models. However, to be clear, all of the methods we’ll talk about (randomization, bootstrap, mathematical models) are techniques to get a sampling distribution.\n\n\n\n\n\n\n\n\nThe sampling distributions we’ve seen so far have been (mostly):\n\n\n\n\n\n\nThis isn’t coincidence…it’s guaranteed by a very important theorem, the Central Limit Theorem.\n\n\n\n\n\n\nTipCentral Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the requirements here?\n\nIndependence:\n\n\n\n\n“Large enough”:\n\n\n\n\n\nNormal Distribution: Nothing follows it exactly–it’s a mathematical construct. But, a lot of things follow it approximately, either:\n\nnaturally:\n\n\n\ncreated to follow it:\n\n\n\nThe normal distribution depends on two parameters, \\(\\mu=\\) mean (where the distribution is centered) and \\(\\sigma=\\) standard deviation (how spread out it is). \\(\\mu\\) shifts the distribution up and down the number line, \\(\\sigma\\) stretches and contracts the curve. The standard normal distribution has \\(\\mu=0\\) and \\(\\sigma=1\\) (this is the distribution tabulated in normal tables in textbooks).\nThe standard normal gives us a convenient way to compare observations, and any normal distribution can be transformed into a standard normal. The Z-score is\n\n\n\n\nIf the Z-score is positive\n\n\nIf the Z-score is negative\nZ-scores can be used to\n\ngauge the unusualness of an observation\n\n\n\nfind probabilities\n\n\n\n\n\nHelpful R functions:\n\npnorm(x, mean=0, sd=1)\nnormTail(m=0,s=1,L=x) or normTail(m=0,s=1,U=x) will draw pretty pictures–need to use the OpenIntro library\nqnorm(prob, mean=0, sd=1) gives a Z-score with area to the left\n\nPictures are super-helpful!\nExample: Full-term birth weights for single babies are normally distributed with a mean of 7.5 pounds and a standard deviation of 1.1 pounds.\n\nA baby is born weighing 9.1 pounds. What is the weight percentile for this baby?\n\n\n\n\n\n\nBabies that weigh less than 5.5 pounds are considered low birth weight. What proportion of babies are low birth weight?\n\n\n\n\n\n\nWhat weight would make a baby at the 25th percentile?\n\n\nWhat is the probability a randomly selected baby weighs between 7 and 8 pounds?\n\n\n\n\n\n\n\n\n\nThe Empirical Rule (aka the 68-95-99.7 Rule) presents a general rule for the probability of falling within one, two, and three standard deviations of the mean in a normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis rule is useful in a wide range of settings when trying to make quick estimate (we’ll use it with bootstraps too!).\nSome more definitions we’ll use throughout the semester:\n\nStandard error:\n\n\n\n\n\n\n\nMargin of error:\n\n\n\n\n\n\nExample (13.11): In 2013, the Pew Research Foundation reported that “45% of US adults report that they live with one ore more chronic conditions.” However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used. Create a 95% confidence interval for the proportion of US adults who live with one or more chronic conditions. Interpret the confidence interval in the context of the study.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section 1 Philosophy.html#decision-errors-chapter-14",
    "href": "Section 1 Philosophy.html#decision-errors-chapter-14",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "1.4 Decision Errors (Chapter 14)",
    "text": "1.4 Decision Errors (Chapter 14)\nAnytime we’re using sample data to make decisions about a larger population we can potentially make a mistake. We can make an incorrect decision in a hypothesis test or calculate a confidence interval that does not capture the true population parameter. In a hypothesis test, there are four possible outcomes:\n\n\n\n\n\n\n\n\n\n\nType I error:\n\n\n\nType II error:\nExamples:\n\nDoping in the Olympics\n\n\n\n\n\n\n\n\n\nCriminal trial\n\n\n\n\n\n\n\n\n\nDiagnostic test for a serious disease\n\n\n\n\n\n\n\n\nErrors require a balancing act. We want to reduce the chance of making a Type I error but this will necessarily increase the chance of making a Type II error. The best we can do is to set the probability of a Type I error. We can do through setting the significance level.\nSignificance level:\n\n\n\n\n\n\nAnother consideration that will impact the chance of making an error is the whether the test is one- or two-sided.\nTwo-sided hypotheses:\n\n\n\n\n\n\nExample: Standard anticoagulant therapy to prevent blood clots requires frequent (expensive) lab monitoring. A new procedure called riva was tested because it did not require frequent monitoring. A randomized trial was conducted in 2012, with standard therapy randomly assigned to 2416 patients and riva randomly assigned to 2416 patients. A bad result was a recurrence of a blood clot in a vein. We want to know if the likelihood of a bad result is different between the two therapies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere are the results of the randomized trial\n\n\n\n\nRiva\nStandard\nTotal\n\n\n\n\nClot\n44\n60\n104\n\n\nNo Clot\n2372\n2356\n4728\n\n\nTotal\n2416\n2416\n4832\n\n\n\n\n\n\n\nFor two-sided tests, the p-value is the probability that we observe a result as least as favorable to the alternative hypothesis as the result we observe. That is, that we observe a result as extreme or more extreme in either direction.\n\n\n\n\n\n\nWhen in doubt, use a two-sided test! Use a one-sided test only if you truly have interest in only one direction.\nSo, how can we control Type I error?\n\nSet up tests before seeing the data.\nCollect enough data that the test has sufficient power. We’ll talk more about power later (and LOTS more in an experimental design course), but power is the probability of correctly rejecting a false null hypothesis. It’s a function of how big the true difference is (which we don’t know and can’t control) and the sample size (which we can control).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section 2 Proportions.html",
    "href": "Section 2 Proportions.html",
    "title": "2  Inference for Proportions (Chapters 16-18)",
    "section": "",
    "text": "2.1 Inference for a Single Proportion (Chapter 16)\nSo far, we’ve discussed randomization, bootstrap, and mathematical models as methods to approximate/describe a sampling distribution and quantify variability. Now, we turn to how these three methods can be used to answer research questions for different kinds of data. The appropriate method will depend on both the type of data and the research question of interest.\nDuring our class, we’ll discuss two types of data: categorical and quantitative. Categorical data arise when the responses are categories. If you think about what is being measured on each unit in the sample, and could imagine checking a box to record the response, the data are categorical. For example:\nWe also have to consider the research question. The research question of interest will drive answers to the following:\nThe answers to these questions will help us determine which method is most appropriate, as well as the specific analysis tool to implement that method. In this unit, we’re going to focus on categorical variables. We’re going to start with a single categorical variable measured on each unit in the sample, where that categorical variable has only two possible outcomes. From there we’ll move to two categorical variables measured on each unit (one explanatory, one response), again with only two possible outcomes for each. Finally, we’ll explore categorical variables with more than two possible outcomes.\nOur first scenario involves a single categorical variable measured on each unit in the sample, with only two possible outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference for Proportions (Chapters 16-18)</span>"
    ]
  },
  {
    "objectID": "Section 2 Proportions.html#inference-for-a-single-proportion-chapter-16",
    "href": "Section 2 Proportions.html#inference-for-a-single-proportion-chapter-16",
    "title": "2  Inference for Proportions (Chapters 16-18)",
    "section": "",
    "text": "2.1.1 Bootstrap Tests for One Proportion\nWhen we discussed bootstrapping earlier, we were sampling (with replacement) from our sample data, because we wanted to understand the variability inherent to our statistic, \\(\\hat p\\), assuming our sample is representative of all samples of the same size that could be drawn from the population. The goal of hypothesis testing is different: we want to understand the sampling distribution of \\(\\hat p\\) under the assumption\n\n\n\n\n\n\nSo, we need to repeatedly sample from a population with \\(p=p_0\\). We can do this by simulating data sets of the same size as original sample, assuming that \\(p=p_0\\). This is called a parametric bootstrap, because we are making an assumption about the value of the underlying parameter \\(p\\) and we are assuming a particular distribution to generate our simulated data sets. From each simulated data set, we could calculate the resulting \\(\\hat p_{\\hbox{\\tiny{sim}}}\\). Many simulated data sets will give us a good approximation for the distribution of \\(\\hat p\\) under our assumptions.\n\nExample: Back to the babies picking the good guy or bad guy. We want to know if babies are more likely to pick the good guy puppet.\n\n\n\n\n\n\n\n\n\nUnder H\\(_0\\), 50% of babies will pick the good guy. We’ll assume this is true for all babies that could be tested. We’ll simulate 16 babies undergoing this test to get a sample proportion from the null distribution.\n\n\n\n\n\n\n\n\n\n\nLet’s see how this works in R. We’ll start by setting up the original data, so we can calculate the test statistic.\n\ndata_baby&lt;-c(rep(1,14),rep(0,2))\n\nobs_prop&lt;-mean(data_baby)\nobs_prop\n\n[1] 0.875\n\n\nNext, we’ll set up the null model with 50% successes and 50% failures:\n\npara_boot&lt;-c(rep(1,8),rep(0,8))\n\npara_boot\n\n [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n\n\nNow, we’ll repeatedly sample from the null distribution, and collect the resulting \\(\\hat p_{pb}\\) from each sample.\n\nnumsim&lt;-100\nboot.sample&lt;-data.frame(sim=1:numsim,stat=NA)\n\nhead(boot.sample)\n\n  sim stat\n1   1   NA\n2   2   NA\n3   3   NA\n4   4   NA\n5   5   NA\n6   6   NA\n\nfor(i in 1:numsim){\n  boot.sample$stat[i]&lt;-mean(sample(para_boot,size=16,replace=TRUE))\n}\n\nhead(boot.sample)\n\n  sim   stat\n1   1 0.7500\n2   2 0.6250\n3   3 0.5625\n4   4 0.5000\n5   5 0.5625\n6   6 0.3750\n\n\nWe can plot these \\(\\hat p_{pb}\\), and see how unusual our observed test statistic is.\n\nlibrary(ggplot2)\nboot.hist&lt;-ggplot(boot.sample, aes(stat)) + geom_histogram(bins=10)\n\nboot.hist\n\n\n\n\n\n\n\n\nWe’ve got a plot, but how do we know exactly how many/what proportion of \\(\\hat p_{pb}\\) were greater than our observed test statistic, \\(\\hat p = 0.875\\)?\n\ncount&lt;-(boot.sample$stat &gt;= obs_prop)\n\nsum(count)\n\n[1] 0\n\nsum(count)/numsim\n\n[1] 0\n\n\nSo, we have an estimated p-value of\n\n\nWhat if we want to change the number of simulated data sets? Let’s try 1000. This gives an estimated p-value of\n\n\n\nWhy is this an estimated p-value?\n\n\n\n\n\n\n\n\nWhy does this histogram not look bell-shaped?\n\n\n\n\n\n\nTry Problem 5 in Chapter 16, and see if you can modify the Babies R code to re-create the histogram provided in the book. It might help to try the applet first and see what has to change there.\n\nWhy Bootstrap?\n\nWorks for any sample size!\nIntuitive way to explain what the p-value is actually measuring.\n\n\n\n2.1.2 Bootstrap Confidence Intervals for One Proportion\nWe’ve already done these! Recall that with a confidence interval, we’re not assuming the null hypothesis is true. Rather, our goal with the bootstrap is to characterize the variability of our statistic \\(\\hat p\\).\n\n\n\n\n\n\n\n\n2.1.3 Mathematical Model for a Proportion\nSometimes, the sampling distribution of \\(\\hat p\\) can be well-approximated using a normal distribution. The conditions which must be met are:\n\n\n\n\n\n\n\n\n\nIf these conditions are met, then\n\n\n\n\n\n\n[Note: This result is just another way of stating the Central Limit Theorem when dealing with proportions! The success/failure condition is playing the role of the “sample is large enough” requirement of the CLT.]\nLet’s think more about the standard error of \\(\\hat p\\). Remember the standard error is\n\n\n\n\n\n\n\nBut this presents a problem. We don’t know \\(p\\) (if we did, we wouldn’t be doing tests or confidence intervals)!\n\n\n\n\n\n\n\n\nHow is this going to play out in hypothesis tests?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Look at Problem 3 in Chapter 16. The journalist claims more than 1/5 adults living in Seattle support defunding the police. Is this true?\n\n\n\n\n\n\n\n\n\n\n\n\nTo find the p-value, we can use R.\n\nnormTail(m=0,s=1,U=2.79)\n\n\n\n\n\n\n\npvalue&lt;-1-pnorm(2.79,mean=0,sd=1)\npvalue\n\n[1] 0.002635402\n\n\nDo you expect the p-value from the parametric bootstrap would be similar? Why or why not?\n\n\n\n\n\n\n\n\nLet’s see.\n\nHow is this going to play out in confidence intervals?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Back to Chapter 16, problem 3. The journalist found that 159/650 Seattle residents support proposals to defund the police.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can change the confidence level by changing \\(z^*\\), and using the qnorm function.\n\n90% confidence\n\n\nqnorm(0.05,mean=0,sd=1)\n\n[1] -1.644854\n\n\n\nBack to the bootstrap confidence interval…\nSo far, we’ve seen bootstrap percentile confidence intervals. We calculated these directly from the bootstrapped \\(\\hat p_{\\hbox{\\tiny{boot}}}\\). If we want a 90% confidence interval, we can find the 5\\(^{th}\\) and 95\\(^{th}\\) percentile values of the \\(\\hat p_{\\hbox{\\tiny{boot}}}\\) values.\nWe can also use the variability of the \\(\\hat p_{\\hbox{\\tiny{boot}}}\\) to calculate an estimate of the standard error of \\(\\hat p\\), and then calculate the interval using the mathematical model approach. This is a bootstrap SE confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a rough approximation, using the 68-95-99.7 Rule which says that 95% of the observed differences should be no farther than 2 SE from the true parameter (\\(p\\)). To do this, the bootstrap histogram must be roughly symmetric and bell-shaped. So, it works for Chapter 16, problem 3; it doesn’t work with the babies.\n\n\nWhy Z-Test/Z Confidence Intervals?\n\nIn many cases, works as well as simulation methods\nEasy to calculate without technology/can do “back of the envelope” analyses\nZ-scores and “estimate \\(\\pm\\) margin of error” are easily interpretable\nClassical methods, used by scientists across disciplines",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference for Proportions (Chapters 16-18)</span>"
    ]
  },
  {
    "objectID": "Section 2 Proportions.html#inference-for-a-comparing-two-proportions-chapter-17",
    "href": "Section 2 Proportions.html#inference-for-a-comparing-two-proportions-chapter-17",
    "title": "2  Inference for Proportions (Chapters 16-18)",
    "section": "2.2 Inference for a Comparing Two Proportions (Chapter 17)",
    "text": "2.2 Inference for a Comparing Two Proportions (Chapter 17)\nWe now move on to consider situations in which two categorical variables are measured on each unit in the sample, and each variable has two possible values. In cases like these, typically one variable is considered the response and one variable is considered explanatory. The explanatory variable may be randomly assigned (like whether or not a subject swam with dolphins) or it may be merely observed (like smoking status). The two possible values of the explanatory variable lead to two groups, and we’re interested in comparing the population proportions that arise from these two groups. We’ll focus on the function of parameters \\(p_1 - p_2\\). The natural estimate of this is \\(\\hat p_1 - \\hat p_2\\): the difference in the sample proportions. We’ll be constructing hypothesis tests to compare \\(p_1\\) to \\(p_2\\) and finding confidence intervals to estimate \\(p_1 - p_2\\).\n\n2.2.1 Randomization tests for the difference in proportions\nExample: Researchers are interested whether electrical brain stimulation will help with problem solving tasks. 40 volunteers were all trained to solve problems in a particular way. Half of the volunteers were randomly assigned to receive electrical stimulation and the other half received a sham stimulation (placebo). All volunteers were then presented with an unfamiliar problem and asked to solve it. The researchers are interested in testing whether the proportion able to solve the problem following electrical stimulation is greater than the proportion able to solve the problem without electrical stimulation.\nThere are a couple of different ways we could state the hypotheses of interest:\n\nH\\(_0:\\)\n\n\n\n\nH\\(_a:\\)\n\n\n\nRecall that hypothesis tests work by assessing how unusual our observed data are, if the null hypothesis is really true. A very unusual result implies that observed data are not likely to have occurred under the null hypothesis. Randomization tests allow us to assess that unusualness by estimating the null distribution–a simulated distribution of what we could expect the distribution of \\(\\hat p_1 - \\hat p_2\\) to look like if H\\(_0\\) is true. We assume H\\(_0\\) is true by recreating the randomization that occurred in the experiment.\nHere are the data:\n\n\n\n\nSolved\nNot Solved\nTotal\n\n\n\n\nSham\n\n\n20\n\n\nElectrical\n\n\n20\n\n\nTotal\n\n\n40\n\n\n\nTo demonstrate what the randomization test is doing, we need 40 cards. Why 40?\n\n\nOf these cards, how many should be red and how many should be black? What do these represent?\n\n\n\nWe’ll shuffle, and deal into two stacks.\n\n\n\n\nA randomization test is going through this shuffling/dealing over and over again, find the difference in proportions for each simulation.\nLet’s look at this in the applets.\nWhat do you notice about the null distribution? How unusual is the observed \\(\\hat p_1 - \\hat p_2\\)?\n\n\n\n\n\n\n\n\nExample: Try Problem 2 in Chapter 17. Set up the hypotheses and describe how a randomization test would work.\n\nHow many cards are needed?\nHow many red? How many black?\nHow many should be dealt into each stack?\nWhat would you calculate from each shuffle/deal?\n\nLet’s do this in R!\n\nFirst, we’ll need to set up the data. This is honestly the hardest part.\n\nVM&lt;-data.frame(Treatment=\"Vaccine\",Response=\"Malaria\",obs=1:89)\nhead(VM)\n\n  Treatment Response obs\n1   Vaccine  Malaria   1\n2   Vaccine  Malaria   2\n3   Vaccine  Malaria   3\n4   Vaccine  Malaria   4\n5   Vaccine  Malaria   5\n6   Vaccine  Malaria   6\n\nVN&lt;-data.frame(Treatment=\"Vaccine\",Response=\"NoMal\",obs=1:203)\nCM&lt;-data.frame(Treatment=\"Control\",Response=\"Malaria\",obs=1:106)\nCN&lt;-data.frame(Treatment=\"Control\",Response=\"NoMal\",obs=1:41)\nmalariadf&lt;-rbind(VM,VN,CM,CN)\nhead(malariadf)\n\n  Treatment Response obs\n1   Vaccine  Malaria   1\n2   Vaccine  Malaria   2\n3   Vaccine  Malaria   3\n4   Vaccine  Malaria   4\n5   Vaccine  Malaria   5\n6   Vaccine  Malaria   6\n\n\nNext, we’ll calculate the observed difference in the proportion of children who contracted malaria between those who received the vaccine and those who received the control. To use the diffmean function, we need to load the mosaic package.\n\nobserved&lt;-diffmean(Response == \"Malaria\" ~ Treatment, data=malariadf)\nobserved\n\n  diffmean \n-0.4162939 \n\n\nNow, we need to shuffle the vaccine/control treatment labels many times and calculate the difference in proportions of malaria for each shuffle. To do this, we’ll need the mosaic library but it’s already been loaded.\n\nmalaria.null&lt;-do(1000)*diffmean(Response == \"Malaria\" ~ shuffle(Treatment),data=malariadf)\nhead(malaria.null)\n\n     diffmean\n1 -0.02765353\n2  0.09507502\n3  0.01325599\n4 -0.02765353\n5 -0.04810828\n6 -0.00719877\n\n\nWe’ll plot the null distribution, and to do this we’ll need the ggplot2 library.\n\nlibrary(ggplot2)\n\nggplot(data=malaria.null) + geom_histogram(mapping=aes(x=diffmean)) + \n  xlab(\"Difference in proportions\") +\n  geom_vline(xintercept = observed, linetype=2, color=\"blue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nFinally, we can calculate the p-value by observing how unusual the observed difference in the proportions of malaria is under the null hypothesis.\n\nprop(~diffmean &lt;= observed, data=malaria.null)\n\nprop_TRUE \n        0 \n\n\nWhy are we considering less than or equal to be more extreme in this example?\n\n\n\n\n\n\n2.2.2 Bootstrap confidence interval for the difference in proportions\nAs we saw with a single proportion, bootstrapping will allow us to estimate the variability of \\(\\hat p_1 - \\hat p_2\\) without assuming the null hypothesis is true. With a single proportion, we drew repeated samples (with replacement) from our sample data, and from each bootstrap sample calculated \\(\\hat p_{\\hbox{\\tiny{boot}}}\\). The distribution of the \\(\\hat p_{\\hbox{\\tiny{boot}}}\\) provided an estimation of the sampling distribution of \\(\\hat p\\).\nNow, with two samples, our observed statistic of interest is\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s go to R, and see how this works with the electrical stimulation example. We’ll start by setting up the data and calculating the observed difference in proportion of solved problems between electrical stimulation and control.\n\n#original Sample 1 data (Electrical), creating a data set with 10 S (1) and 10 F (0)#\nelectrical&lt;-c(rep(1,10),rep(0,10))\n\n#original Sample 2 data (Control), creating a data set with 6 S (1) and 14 F (0)#\ncontrol&lt;-c(rep(1,6),rep(0,14))\n\n#Calculate and print the observed statistic, p-hat_E - p-hat_C#\nobs_stat&lt;-mean(electrical)-mean(control)\nobs_stat\n\n[1] 0.2\n\n\nNow, we’ll repeatedly sample with replacement separately from each group. First we need to set up a place to store our summaries from each resample.\n\n#Set up an empty data set with 4 columns: sim number, p_hat_boot_E, p_hat_boot_C, diff#\n\nboot.samples&lt;-data.frame(sim=1:1000,stat_E=NA,stat_C=NA, diff=NA)\n\nhead(boot.samples)\n\n  sim stat_E stat_C diff\n1   1     NA     NA   NA\n2   2     NA     NA   NA\n3   3     NA     NA   NA\n4   4     NA     NA   NA\n5   5     NA     NA   NA\n6   6     NA     NA   NA\n\n#For each row in the data set, draw a bootstrap sample from Sample 1 and Sample 2 and find#\n# p_hat_boot_E and p_hat_boot_C #\n\nfor(i in 1:1000){\n  boot.samples$stat_E[i]&lt;-mean(sample(electrical,size=20,replace=TRUE))\n  boot.samples$stat_C[i]&lt;-mean(sample(control,size=20,replace=TRUE))\n}\n\nhead(boot.samples)\n\n  sim stat_E stat_C diff\n1   1   0.60   0.45   NA\n2   2   0.45   0.35   NA\n3   3   0.35   0.25   NA\n4   4   0.45   0.45   NA\n5   5   0.30   0.40   NA\n6   6   0.65   0.35   NA\n\n#Now find the differences#\n\nboot.samples$diff&lt;-boot.samples$stat_E - boot.samples$stat_C\n\nhead(boot.samples)\n\n  sim stat_E stat_C  diff\n1   1   0.60   0.45  0.15\n2   2   0.45   0.35  0.10\n3   3   0.35   0.25  0.10\n4   4   0.45   0.45  0.00\n5   5   0.30   0.40 -0.10\n6   6   0.65   0.35  0.30\n\n\nNow let’s plot the bootstrap distribution.\n\nboot.hist&lt;-ggplot(boot.samples, aes(diff)) + geom_histogram(binwidth=0.05)\n\nboot.hist\n\n\n\n\n\n\n\n\nNotice where this distribution is centered!\n\n\n\nWe are re-using this example, since we’ve already carried out the randomization test in the applet. By doing so we are ignoring (maybe) the research question. But, we’re doing both a randomization test and confidence interval so that we can compare the resulting sampling distributions of \\(\\hat p_1 - \\hat p_2\\). What is different? What’s the same?\n\n\n\n\n\n\nNow that we have the bootstrap distribution, we can find the bootstrap percentile confidence interval. Let’s do a 90% interval.\n\n#start by ranking the bootstrap differences from smallest to largest #\nrankdiff&lt;-sort(boot.samples$diff)\n\n#Print out just the first few#\nhead(rankdiff)\n\n[1] -0.30 -0.25 -0.25 -0.25 -0.20 -0.20\n\n#Lower endpoint is the 5th percentile (90% confidence)#\nlower&lt;-rankdiff[50]\nlower\n\n[1] -0.05\n\n#Upper endpoint is the 950th percentile (90% confidence)#\nupper&lt;-rankdiff[950]\nupper\n\n[1] 0.45\n\n\n\n\n\n\n\nBecause our bootstrap distribution is relatively bell-shaped, we could also a calculate a rough 95% boostrap SE confidence interval.\n\nSE&lt;-sd(rankdiff)\nSE\n\n[1] 0.1520223\n\n\n\n\n\n\n\n\n\n\n2.2.3 Mathematical model for the difference in proportions\nFor a single proportion, we needed two conditions to be met to ensure the sampling distribution of \\(\\hat p\\) is approximately normal:\n\n\n\n\n\n\n\nIf these conditions are met, then\n\n\n\n\nWe must meet similar conditions to ensure the sampling distribution of \\(\\hat p_1 - \\hat p_2\\) is approximately normal:\n\n\n\n\n\n\n\n\nIf these conditions are met, then\n\n\n\n\n\n\n\n\n\n\nLike before we don’t know \\(p_1\\) and \\(p_2\\), so we’ll use our best guess. And, like before, our best guess will change depending on whether we’re constructing a confidence interval or carrying out a hypothesis test.\nHow is this going to play out in a hypothesis test?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample (17.5): (Aside: why can’t we do the electrical stimuation example again?). A 2021 Gallup poll surveyed 3941 students pursuing a bachelor’s degree and 2064 students pursuing an associate’s degree. The survey found that 51% of the bachelor’s students (2010) and 44% of the associate’s students (908) said that COVID-19 will negatively impact their ability to complete the degree. We want to decide whether the proportion of bachelor’s students who believe the pandemic will negatively impact degree completion is different from the proportion of associate’s students who believe they will be negatively affected. Let \\(p_B\\) be the proportion of bachelor’s students who believe they’ll be negatively affected and let \\(p_A\\) be the proportion of associate’s students who believe they’ll be negatively affected.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo visualize the p-value:\n\nnormTail(m=0,s=1,L=-5.15, U=5.15)\n\n\n\n\n\n\n\n\nAnd to get the p-value:\n\npnorm(-5.15,mean=0,sd=1)+(1-pnorm(5.15,mean=0,sd=1))\n\n[1] 2.604865e-07\n\n\n\n\n\n\n\n\n\nHow is this going to play out in a confidence interval?\n\n\n\n\n\n\n\n\nExample(17.9): A Kaiser Family Foundation poll for US adults in 2019 found that 79% of Democrats, 55% of Independents, and 24% of Republicans supported a generic “National Health Plan.” There were 347 Democrats, 298 Republicans, and 617 Independents surveyed (Foundation, 2019). We want to estimate the difference between the proportion of Democrats and Independents who support a National Health Plan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat impacts the width of a confidence interval?\nThere are three main things that impact the width of a confidence interval:\n\nConfidence level\n\n\n\n\nSample size\n\n\n\n\nStandard Error\n\n\n\nNo matter what method we use to calculate the confidence interval, the confidence level is a statement about the long run percentage of confidence intervals that would succeed in capturing the true value of the parameter. What does this mean? Applet\n\n\nHypothesis tests vs. Confidence Intervals\nWhile we should be matching analysis method to the research question, there is a nice relationship between hypothesis tests and confidence intervals. Recall that confidence intervals give a set of plausible values for the unknown parameter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference for Proportions (Chapters 16-18)</span>"
    ]
  },
  {
    "objectID": "Section 2 Proportions.html#inference-for-two-way-tables-chapter-18",
    "href": "Section 2 Proportions.html#inference-for-two-way-tables-chapter-18",
    "title": "2  Inference for Proportions (Chapters 16-18)",
    "section": "2.3 Inference for Two-Way Tables (Chapter 18)",
    "text": "2.3 Inference for Two-Way Tables (Chapter 18)\nSo far, we’ve considered categorical variables with only two possible outcomes: success and failure. Many categorical variables have more than two possible outcomes, so we can’t easily define the proportion of “successes.” Instead, we’ll summarize categorical data with more than two levels using two-way tables. In this class, we’re still going to restrict ourselves to only two variables (often explanatory and response, but not necessarily), both with two or more levels. However, there are certainly statistical methods for more complicated situations.\nTypically, research questions focus on how the proportions of the possible outcomes in the response variable change (or don’t) across the levels of the explanatory variable. However, we can also consider questions about a single variable with more than two outcomes (are the possible outcomes all equally likely? do the possible outcomes follow a particular pattern?) or just whether the two categorical variables are independent or dependent without assigning an explanatory/response relationship. Due to the structure of the variable(s), there really isn’t a population parameter of interest. We can’t (usually) make a function of proportion of successes that makes sense to estimate, like we can with \\(p_1 - p_2\\). That means we’ll be considering only tests, not confidence intervals. We’ll focus on the randomization test and the mathematical model approach. Both methods start with the same set-up.\n\nExample: When surveys are administered, we hope that the respondents give accurate answers. Does the mode of survey delivery affect this? Schober et al (2015) investigated this question. They had 147 people who agreed to be interviewed on an iPhone, and they were randomly assigned to one of three interview modes: human voice, automated voice, text. One question asked was whether they exercise less than once per week during a typical week (a yes is mostly likely considered socially undesirable). The explanatory variable here is survey mode and the response is whether or not the respondent said yes. Here are the data:\n\n\n\n\nText\nHuman Voice\nAutomated Voice\nTotal\n\n\n\n\nExercise Yes\n34\n21\n30\n75\n\n\nExercise No\n124\n139\n139\n402\n\n\nTotal\n158\n160\n159\n477\n\n\n\nBased on these data, it looks like the answer to the question does change depending on survey mode, with respondents more likely to say yes via text. However, we don’t know if this result could have happened by chance.\n\n\n\n\n\n\n\n\n\n\n2.3.1 Expected Counts\nWe don’t expect the proportion of `yes’ to be exactly the same across all survey modes, but we want to know if these vary enough to convince us that survey mode and answer are not independent. To do this, we need to find for each cell in the table.\n\nAgain, here are the observed data\n\n\n\n\nText\nHuman Voice\nAutomated Voice\nTotal\n\n\n\n\nExercise Yes\n34\n21\n30\n75\n\n\nExercise No\n124\n139\n139\n402\n\n\nTotal\n158\n160\n159\n477\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText\nHuman Voice\nAuto Voice\nTotal\n\n\n\n\nExercise Yes\n34 (_____)\n21 (_____)\n30 (_____)\n75\n\n\nExercise No\n124 (_____)\n139 (_____)\n139 (_____)\n402\n\n\nTotal\n158\n160\n159\n477\n\n\n\n\nSo now the key question…are the observed and expected cell counts different enough?\n\nCell(1,1) obs - exp = 34 -\nCell(1,2) obs - exp = 21 -\nCell(1,3) obs - exp = 20 -\nCell(2,1) obs - exp = 124 -\nCell(2,2) obs - exp = 139 -\nCell(2,3) obs - exp = 139 -\n\n\n\n\n\n\nNew test statistic!\n\n\n\n\n\n\nIn our example:\n\nCell(1,1) (obs - exp)\\(^2\\)/exp = \\((34 - 24.84)^2/(24.84) = 9.16^2/24.84 = 3.3778\\)\nCell(1,2) (obs - exp)\\(^2\\)/exp = \\((21 - 25.16)^2/(25.16) = (-4.16)^2/25.16 = 0.6878\\)\nCell(1,3) (obs - exp)\\(^2\\)/exp = \\((20 - 25)^2/(25) = (-5)^2/25 = 1\\)\nCell(2,1) (obs - exp)\\(^2\\)/exp = \\((124 - 133.16)^2/(133.16) = (-9.16)^2/133.16 = 0.6301\\)\nCell(2,2) (obs - exp)\\(^2\\)/exp = \\((139 - 134.84)^2/(134.84) = 4.16^2/134.84 = 0.1283\\)\nCell(2,3) (obs - exp)\\(^2\\)/exp = \\((139 - 134)^2/(134) = 5^2/134 =0.3731\\)\n\n\n\n\nTo see if this is ‘big’ we need the sampling distribution of our new test statistic. We can estimate that sampling distribution using either a randomization test or the mathematical model approach.\n\n\n\n2.3.2 Randomization Test\nThe randomization test for a two-way table works just like it does with two samples. We’ll randomize by shuffling and dealing/assigning the 75 yes answers and 402 no answers to the three survey modes at random.\n\nHow many colors of cards?\n\n\n\n\n\nHow many stacks to deal them into? How many in each stack?\n\n\n\n\n\nWhat do we find for each deal/shuffle?\n\n\n\n\nApplet\n\n\n\n\n\n\n\nConclusion:\n\n\n\n2.3.3 Mathematical Model\nBased on what we just observed in the applet, the normal distribution is not going to be a good approximation to sampling distribution. It turns out this test statistic follows a different mathematical distribution, the chi-squared distribution (proof: see STAT 462). The normal distribution has two parameters that determine its shape: the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The shape of the chi-square distribution is determined by a parameter called the . Figure 18.2 on page 307 shows how the shape of the distribution changes depending on the df.\nSo how can we use this?\n\n\n\n\n\n\n\n\n\nAgain, we have conditions that need to be met for the mathematical model to be a good approximation:\n\n\n\n\n\n\nExample: Let’s go back and do the survey mode example using the mathematical model approach. First, we’ll need to check the conditions are met:\n\n\n\n\n\n\n\n\nTo find the p-value, we can use the R function pchisq(). Like pnorm it gives area to the left. So,\n\npchisq(6.1971,df=2,lower.tail=FALSE)\n\n[1] 0.04511457\n\n\n\nWe can also do this directly in R.\n\nsurveymode&lt;-read.csv(\"surveymode.csv\",header=TRUE)\n\n#make a table#\nmode&lt;-table(surveymode$Response,surveymode$Mode)\n\n#see the table, note alphabetical order#\nmode\n\n     \n      Avoice Hvoice Text\n  No     139    139  124\n  Yes     20     21   34\n\n#Chi-square mathematical model test#\nchisq.test(mode)\n\n\n    Pearson's Chi-squared test\n\ndata:  mode\nX-squared = 6.0069, df = 2, p-value = 0.04962\n\n#Chi-square randomization test#\nchisq.test(mode,simulate.p.value=TRUE, B=1000)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 1000\n    replicates)\n\ndata:  mode\nX-squared = 6.0069, df = NA, p-value = 0.04595",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference for Proportions (Chapters 16-18)</span>"
    ]
  }
]
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 102 Notes",
    "section": "",
    "text": "Course Goals for STAT 102\nSTAT 102 is an introduction to formal statistical inference. We will carry out inference using both simulation-based approaches and classical, theory-based methods. By the end of the course, you will:\n\nRead an example where the research question is explicitly stated, and then translate what’s stated into a statistical statement involving parameters or other simple distributional characteristics.\nIdentify whether the ideal data collection strategy would involve random assignment, random sampling, or both and explain why.\nWork with an example where the research question is explicitly stated, along with an existing data set, and propose and carry out an appropriate analysis to answer the research question.\nExplain the terms/components of a given statistical model, and connect those terms to the research question at hand.\nCheck basic assumptions of various (simple) analysis methods and justify the use of the method.\nApply existing functions and point-and-click software for implementing basic data analyses.\nUse tactile simulation to carry out a simple resampling procedure.\nIdentify the steps and perform the calculations required for routine statistical procedures to address a given problem.\nCalculate simple analyses (t-test, chi-squared test for proportions) by hand, to verify the validity of the computational algorithm.\nRecognize when computational results do not make sense in the context of the problem.",
    "crumbs": [
      "Course Goals for STAT 102"
    ]
  },
  {
    "objectID": "Section-1-Philosophy.html",
    "href": "Section-1-Philosophy.html",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "",
    "text": "1.1 Randomization Tests (Chapter 11)\nIn STAT 101, you focused on Exploratory Data Analysis. Exploratory data analysis aims to investigate the characteristics of a data set through visualizations and numerical summaries. Visualizations may include:\nNumerical summaries used to explore a data set may include:\nMore often than not, the data were collected to answer a research question about a larger population for which the data collected are a (hopefully) representative sample. This notion of drawing conclusions beyond the data collected is at the heart of statistical inference.\nExample: Bred in the Bone\nTake away:\nIn exploratory data analysis, the visualizations and numerical summaries you choose are driven by the type of data at hand. This is true for statistical inference as well. The type of data will drive the appropriate inference techniques. However, the goal of the research study will also impact the selected method, as will the underlying assumptions of the technique (we’ll talk a lot more about this). That said, there are some overarching approaches to quantifying variability, and thus drawing conclusions beyond the data set at hand.\nApproaches to quantifying variability\nWe’ll start the semester by talking about these three approaches fairly generally. For (most of) the rest of the semester, we’ll see how these approaches fit with different types of data.\nThe goal of hypothesis tests is to use an observed data set to answer a yes/no question about a characteristic of a larger population from which the observed data set was drawn. For example, is swimming with dolphins therapeutic for patients with clinical depression? That is, we want to assess whether or not the explanatory variable causes changes in the response variable.\nTo answer this question, Antonioli and Reveley (2005) recruited 30 subjects with a clinical diagnosis of mild to moderate depression. The subjects were required to stop all other treatments (therapy and/or pharmaceuticals) 4 weeks prior the experiment, and the 30 subjects were all taken to an island off the coast of Honduras. The subjects were randomly assigned to one of two groups. Both groups spent one hour swimming and snorkeling each day, but one group did so in the presence of dolphins and the other group did not. At the end of two weeks, each subject’s level of depressions was evaluated, and whether or not the subjects had a substantial improvement in their depression was recorded.\nExplanatory variable:\nResponse variable:\nIs this an observational study or an experiment? What does that imply about inference?\nThe question we will answer is whether the resulting data provide convincing evidence that subjects who swam with dolphins were more likely to see depression improvement than subjects who swam without dolphins.\nIf there really is no impact of swimming with dolphins, what does this imply about the explanatory and response variables?\nIf swimming with dolphins does improve depression, what does this imply about the explanatory and response variables?\nThis leads to two competing claims:\nIf the null hypothesis is true, how would this manifest in the observed data?\nIf the alternative hypothesis is true, how would this manifest in the observed data?\nWe will choose between the competing claims by assessing whether the data conflict so much with H\\(_0\\) that the null hypothesis cannot be considered reasonable. If this happens, we’ll reject the notion of H\\(_0\\) and conclude that H\\(_a\\) must be true.\nUp to now, we haven’t seen the data! Here’s a summary:\nWe can see that\nSo,\nThe question remains…is this enough different from what we would expect under the null hypothesis to conclude that swimming with dolphins does make a difference in depression?\nSo far, nothing we’ve laid out is unique to a randomization test. Where does randomization come in?\nLet’s visualize these observations as a set of cards. Each card denotes a subject in the study. The color indicates the response: red for substantial improvement and black for no substantial improvement.\nAny difference we see in the simulation is due to chance–the cards were randomly dealt into the dolphin/control groups.\nIt’s not realistic to keep shuffling and dealing by hand…we need to turn to technology to do the randomization for us: Applet\nWe can do this over and over again to build up a null distribution. This distribution shows how we expect the variability to behave under the null hypothesis:\nWhat do you notice about this null distribution?\nHow rare is it to see our observed statistic 0.467 in this distribution? What does this imply?\nSo, we’ve just carried out a statistical inference technique! We might be wrong in our conclusions (more on this in Chapter 14), but we’ve made the best decision we could with the data available.\nIn summary:\nRandomization Test Procedure:\nNow let’s go to R!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section-1-Philosophy.html#randomization-tests-chapter-11",
    "href": "Section-1-Philosophy.html#randomization-tests-chapter-11",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "",
    "text": "Null hypothesis: H\\(_0\\)\n\n\n\nAlternative hypothesis: H\\(_a\\)\n\n\n\n\n\n\n\n\n\n\n\n\nDolphin Therapy\nControl Group\nTotal\n\n\n\n\nShowed Improvement\n\n\n\n\n\nNo Improvement\n\n\n\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne Shuffle\n\n\n\n\n\n\n100 Shuffles\n\n\n\n\n\n\n\n\n\n\nFrame the research question in terms of hypotheses\nCollect data from an observational study or experiment\nModel randomness that would occur if H\\(_0\\) is true\nAnalyze the data by comparing the observed data to the simulated distribution\nConclusion",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section-1-Philosophy.html#bootstrap-methods-chapter-12",
    "href": "Section-1-Philosophy.html#bootstrap-methods-chapter-12",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "1.2 Bootstrap Methods (Chapter 12)",
    "text": "1.2 Bootstrap Methods (Chapter 12)\nBootstrap methods are a relatively new statistical technique (proposed in 1979 by Efron), but they are based on a very simple idea. The goal is to characterize the variability of the statistic across many samples. One way we could do this is take lots and lots of samples from the population, and get a picture of how much variance there is among the samples. This is almost always impossible. So, rather than resample from the population, we could try resampling from the sample. This is the basic idea behind the bootstrap.\nBootstrapping is used in many different applications. For this general introduction to the approach, we’re going to consider a confidence interval for a proportion.\nA confidence interval is\n\n\n\n\n\nNote the goal of the confidence interval is different from the goal of a hypothesis test!\n\n\n\n\nHowever, like with hypothesis tests, we need to understand the variability inherent to the statistic. To figure out how wide the range of plausible values should be, we need to know how a statistic varies from sample to sample in the population.\nFor example, let’s think back to the Baby scenario and suppose our goal is to estimate the population parameter\n\n\n\n\nThe researchers collected one sample of 16 babies, and found that 14 picked the good guy. This is our observed data. What do you think would happen if we took a sample of 16 different babies? And then a different sample of 16 babies?\n\n\n\n\nIdea of the bootstrap:\n\n\n\n\n\n\n\nInfinite populations are pretty tough to work with, though. However, we can produce an equivalent bootstrap distribution by\n\n\n\n\nSo, we’ll repeatedly draw bootstrap samples of size 16 (why 16?) and calculate the proportion of successes in each bootstrap samples. After we do this many many times, we’ll have an idea of a range of plausible values for the population parameter. We’ll set the confidence level by opting for a wider or a narrower interval, based on how certain we need to be in the results.\n\n\n\n\nBootstrap Process\n\nFrame the research question in terms of a parameter to estimate\nCollect data using an observational study or an experiment\nModel the randomness by using the observed data as a proxy for the population\nCreate the interval (in future chapters we’ll see there are multiple ways to do this)\nConclusion\n\nLet’s go to R!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section-1-Philosophy.html#inference-with-mathematical-models-chapter-13",
    "href": "Section-1-Philosophy.html#inference-with-mathematical-models-chapter-13",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "1.3 Inference with Mathematical Models (Chapter 13)",
    "text": "1.3 Inference with Mathematical Models (Chapter 13)\nSo far, we’ve seen computational methods like randomization and bootstrapping to characterize the variability of a statistic. The use of computational methods is relatively recent, due to the increase in computing power. In pre-computing days, re-sampling and randomization was very difficult. As a result, mathematical approximations were used and are still pervasive. If you took AP Statistics or a different intro statistics course, you employed mathematical models. However, to be clear, all of the methods we’ll talk about (randomization, bootstrap, mathematical models) are techniques to get a sampling distribution.\n\n\n\n\n\n\n\n\nThe sampling distributions we’ve seen so far have been (mostly):\n\n\n\n\n\n\nThis isn’t coincidence…it’s guaranteed by a very important theorem, the Central Limit Theorem.\n\n\n\n\n\n\nTipCentral Limit Theorem\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the requirements here?\n\nIndependence:\n\n\n\n\n“Large enough”:\n\n\n\n\n\nNormal Distribution: Nothing follows it exactly–it’s a mathematical construct. But, a lot of things follow it approximately, either:\n\nnaturally:\n\n\n\ncreated to follow it:\n\n\n\nThe normal distribution depends on two parameters, \\(\\mu=\\) mean (where the distribution is centered) and \\(\\sigma=\\) standard deviation (how spread out it is). \\(\\mu\\) shifts the distribution up and down the number line, \\(\\sigma\\) stretches and contracts the curve. The standard normal distribution has \\(\\mu=0\\) and \\(\\sigma=1\\) (this is the distribution tabulated in normal tables in textbooks).\nThe standard normal gives us a convenient way to compare observations, and any normal distribution can be transformed into a standard normal. The Z-score is\n\n\n\n\nIf the Z-score is positive\n\n\nIf the Z-score is negative\nZ-scores can be used to\n\ngauge the unusualness of an observation\n\n\n\nfind probabilities\n\n\n\n\n\nHelpful R functions:\n\npnorm(x, mean=0, sd=1)\nnormTail(m=0,s=1,L=x) or normTail(m=0,s=1,U=x) will draw pretty pictures–need to use the OpenIntro library\nqnorm(prob, mean=0, sd=1) gives a Z-score with area to the left\n\nPictures are super-helpful!\nExample: Full-term birth weights for single babies are normally distributed with a mean of 7.5 pounds and a standard deviation of 1.1 pounds.\n\nA baby is born weighing 9.1 pounds. What is the weight percentile for this baby?\n\n\n\n\n\n\nBabies that weigh less than 5.5 pounds are considered low birth weight. What proportion of babies are low birth weight?\n\n\n\n\n\n\nWhat weight would make a baby at the 25th percentile?\n\n\nWhat is the probability a randomly selected baby weighs between 7 and 8 pounds?\n\n\n\n\n\n\n\n\n\nThe Empirical Rule (aka the 68-95-99.7 Rule) presents a general rule for the probability of falling within one, two, and three standard deviations of the mean in a normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis rule is useful in a wide range of settings when trying to make quick estimate (we’ll use it with bootstraps too!).\nSome more definitions we’ll use throughout the semester:\n\nStandard error:\n\n\n\n\n\n\n\nMargin of error:\n\n\n\n\n\n\nExample (13.11): In 2013, the Pew Research Foundation reported that “45% of US adults report that they live with one ore more chronic conditions.” However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used. Create a 95% confidence interval for the proportion of US adults who live with one or more chronic conditions. Interpret the confidence interval in the context of the study.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section-1-Philosophy.html#decision-errors-chapter-14",
    "href": "Section-1-Philosophy.html#decision-errors-chapter-14",
    "title": "1  Philosophy of Statistical Inference (Chapters 11-14)",
    "section": "1.4 Decision Errors (Chapter 14)",
    "text": "1.4 Decision Errors (Chapter 14)\nAnytime we’re using sample data to make decisions about a larger population we can potentially make a mistake. We can make an incorrect decision in a hypothesis test or calculate a confidence interval that does not capture the true population parameter. In a hypothesis test, there are four possible outcomes:\n\n\n\n\n\n\n\n\n\n\nType I error:\n\n\n\nType II error:\nExamples:\n\nDoping in the Olympics\n\n\n\n\n\n\n\n\n\nCriminal trial\n\n\n\n\n\n\n\n\n\nDiagnostic test for a serious disease\n\n\n\n\n\n\n\n\nErrors require a balancing act. We want to reduce the chance of making a Type I error but this will necessarily increase the chance of making a Type II error. The best we can do is to set the probability of a Type I error. We can do through setting the significance level.\nSignificance level:\n\n\n\n\n\n\nAnother consideration that will impact the chance of making an error is the whether the test is one- or two-sided.\nTwo-sided hypotheses:\n\n\n\n\n\n\nExample: Standard anticoagulant therapy to prevent blood clots requires frequent (expensive) lab monitoring. A new procedure called riva was tested because it did not require frequent monitoring. A randomized trial was conducted in 2012, with standard therapy randomly assigned to 2416 patients and riva randomly assigned to 2416 patients. A bad result was a recurrence of a blood clot in a vein. We want to know if the likelihood of a bad result is different between the two therapies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere are the results of the randomized trial\n\n\n\n\nRiva\nStandard\nTotal\n\n\n\n\nClot\n44\n60\n104\n\n\nNo Clot\n2372\n2356\n4728\n\n\nTotal\n2416\n2416\n4832\n\n\n\n\n\n\n\nFor two-sided tests, the p-value is the probability that we observe a result as least as favorable to the alternative hypothesis as the result we observe. That is, that we observe a result as extreme or more extreme in either direction.\n\n\n\n\n\n\nWhen in doubt, use a two-sided test! Use a one-sided test only if you truly have interest in only one direction.\nSo, how can we control Type I error?\n\nSet up tests before seeing the data.\nCollect enough data that the test has sufficient power. We’ll talk more about power later (and LOTS more in an experimental design course), but power is the probability of correctly rejecting a false null hypothesis. It’s a function of how big the true difference is (which we don’t know and can’t control) and the sample size (which we can control).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Philosophy of Statistical Inference (Chapters 11-14)</span>"
    ]
  },
  {
    "objectID": "Section-2-Proportions.html",
    "href": "Section-2-Proportions.html",
    "title": "2  Inference for Proportions (Chapters 16-18)",
    "section": "",
    "text": "2.1 Inference for a Single Proportion (Chapter 16)\nSo far, we’ve discussed randomization, bootstrap, and mathematical models as methods to approximate/describe a sampling distribution and quantify variability. Now, we turn to how these three methods can be used to answer research questions for different kinds of data. The appropriate method will depend on both the type of data and the research question of interest.\nDuring our class, we’ll discuss two types of data: categorical and quantitative. Categorical data arise when the responses are categories. If you think about what is being measured on each unit in the sample, and could imagine checking a box to record the response, the data are categorical. For example:\nWe also have to consider the research question. The research question of interest will drive answers to the following:\nThe answers to these questions will help us determine which method is most appropriate, as well as the specific analysis tool to implement that method. In this unit, we’re going to focus on categorical variables. We’re going to start with a single categorical variable measured on each unit in the sample, where that categorical variable has only two possible outcomes. From there we’ll move to two categorical variables measured on each unit (one explanatory, one response), again with only two possible outcomes for each. Finally, we’ll explore categorical variables with more than two possible outcomes.\nOur first scenario involves a single categorical variable measured on each unit in the sample, with only two possible outcomes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference for Proportions (Chapters 16-18)</span>"
    ]
  },
  {
    "objectID": "Section-2-Proportions.html#inference-for-a-single-proportion-chapter-16",
    "href": "Section-2-Proportions.html#inference-for-a-single-proportion-chapter-16",
    "title": "2  Inference for Proportions (Chapters 16-18)",
    "section": "",
    "text": "2.1.1 Bootstrap Tests for One Proportion\nWhen we discussed bootstrapping earlier, we were sampling (with replacement) from our sample data, because we wanted to understand the variability inherent to our statistic, \\(\\hat p\\), assuming our sample is representative of all samples of the same size that could be drawn from the population. The goal of hypothesis testing is different: we want to understand the sampling distribution of \\(\\hat p\\) under the assumption\n\n\n\n\n\n\nSo, we need to repeatedly sample from a population with \\(p=p_0\\). We can do this by simulating data sets of the same size as original sample, assuming that \\(p=p_0\\). This is called a parametric bootstrap, because we are making an assumption about the value of the underlying parameter \\(p\\) and we are assuming a particular distribution to generate our simulated data sets. From each simulated data set, we could calculate the resulting \\(\\hat p_{\\hbox{\\tiny{sim}}}\\). Many simulated data sets will give us a good approximation for the distribution of \\(\\hat p\\) under our assumptions.\n\nExample: Back to the babies picking the good guy or bad guy. We want to know if babies are more likely to pick the good guy puppet.\n\n\n\n\n\n\n\n\n\nUnder H\\(_0\\), 50% of babies will pick the good guy. We’ll assume this is true for all babies that could be tested. We’ll simulate 16 babies undergoing this test to get a sample proportion from the null distribution.\n\n\n\n\n\n\n\n\n\n\nLet’s see how this works in R. We’ll start by setting up the original data, so we can calculate the test statistic.\n\ndata_baby&lt;-c(rep(1,14),rep(0,2))\n\nobs_prop&lt;-mean(data_baby)\nobs_prop\n\n[1] 0.875\n\n\nNext, we’ll set up the null model with 50% successes and 50% failures:\n\npara_boot&lt;-c(rep(1,8),rep(0,8))\n\npara_boot\n\n [1] 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n\n\nNow, we’ll repeatedly sample from the null distribution, and collect the resulting \\(\\hat p_{pb}\\) from each sample.\n\nnumsim&lt;-100\nboot.sample&lt;-data.frame(sim=1:numsim,stat=NA)\n\nhead(boot.sample)\n\n  sim stat\n1   1   NA\n2   2   NA\n3   3   NA\n4   4   NA\n5   5   NA\n6   6   NA\n\nfor(i in 1:numsim){\n  boot.sample$stat[i]&lt;-mean(sample(para_boot,size=16,replace=TRUE))\n}\n\nhead(boot.sample)\n\n  sim   stat\n1   1 0.3750\n2   2 0.7500\n3   3 0.6250\n4   4 0.5000\n5   5 0.7500\n6   6 0.5625\n\n\nWe can plot these \\(\\hat p_{pb}\\), and see how unusual our observed test statistic is.\n\nlibrary(ggplot2)\nboot.hist&lt;-ggplot(boot.sample, aes(stat)) + geom_histogram(bins=10)\n\nboot.hist\n\n\n\n\n\n\n\n\nWe’ve got a plot, but how do we know exactly how many/what proportion of \\(\\hat p_{pb}\\) were greater than our observed test statistic, \\(\\hat p = 0.875\\)?\n\ncount&lt;-(boot.sample$stat &gt;= obs_prop)\n\nsum(count)\n\n[1] 0\n\nsum(count)/numsim\n\n[1] 0\n\n\nSo, we have an estimated p-value of\n\n\nWhat if we want to change the number of simulated data sets? Let’s try 1000. This gives an estimated p-value of\n\n\n\nWhy is this an estimated p-value?\n\n\n\n\n\n\n\n\nWhy does this histogram not look bell-shaped?\n\n\n\n\n\n\nTry Problem 5 in Chapter 16, and see if you can modify the Babies R code to re-create the histogram provided in the book. It might help to try the applet first and see what has to change there.\n\nWhy Bootstrap?\n\nWorks for any sample size!\nIntuitive way to explain what the p-value is actually measuring.\n\n\n\n2.1.2 Bootstrap Confidence Intervals for One Proportion\nWe’ve already done these! Recall that with a confidence interval, we’re not assuming the null hypothesis is true. Rather, our goal with the bootstrap is to characterize the variability of our statistic \\(\\hat p\\).\n\n\n\n\n\n\n\n\n2.1.3 Mathematical Model for a Proportion\nSometimes, the sampling distribution of \\(\\hat p\\) can be well-approximated using a normal distribution. The conditions which must be met are:\n\n\n\n\n\n\n\n\n\nIf these conditions are met, then\n\n\n\n\n\n\n[Note: This result is just another way of stating the Central Limit Theorem when dealing with proportions! The success/failure condition is playing the role of the “sample is large enough” requirement of the CLT.]\nLet’s think more about the standard error of \\(\\hat p\\). Remember the standard error is\n\n\n\n\n\n\n\nBut this presents a problem. We don’t know \\(p\\) (if we did, we wouldn’t be doing tests or confidence intervals)!\n\n\n\n\n\n\n\n\nHow is this going to play out in hypothesis tests?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Look at Problem 3 in Chapter 16. The journalist claims more than 1/5 adults living in Seattle support defunding the police. Is this true?\n\n\n\n\n\n\n\n\n\n\n\n\nTo find the p-value, we can use R.\n\nnormTail(m=0,s=1,U=2.79)\n\n\n\n\n\n\n\npvalue&lt;-1-pnorm(2.79,mean=0,sd=1)\npvalue\n\n[1] 0.002635402\n\n\nDo you expect the p-value from the parametric bootstrap would be similar? Why or why not?\n\n\n\n\n\n\n\n\nLet’s see.\n\nHow is this going to play out in confidence intervals?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Back to Chapter 16, problem 3. The journalist found that 159/650 Seattle residents support proposals to defund the police.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can change the confidence level by changing \\(z^*\\), and using the qnorm function.\n\n90% confidence\n\n\nqnorm(0.05,mean=0,sd=1)\n\n[1] -1.644854\n\n\n\nBack to the bootstrap confidence interval…\nSo far, we’ve seen bootstrap percentile confidence intervals. We calculated these directly from the bootstrapped \\(\\hat p_{\\hbox{\\tiny{boot}}}\\). If we want a 90% confidence interval, we can find the 5\\(^{th}\\) and 95\\(^{th}\\) percentile values of the \\(\\hat p_{\\hbox{\\tiny{boot}}}\\) values.\nWe can also use the variability of the \\(\\hat p_{\\hbox{\\tiny{boot}}}\\) to calculate an estimate of the standard error of \\(\\hat p\\), and then calculate the interval using the mathematical model approach. This is a bootstrap SE confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a rough approximation, using the 68-95-99.7 Rule which says that 95% of the observed differences should be no farther than 2 SE from the true parameter (\\(p\\)). To do this, the bootstrap histogram must be roughly symmetric and bell-shaped. So, it works for Chapter 16, problem 3; it doesn’t work with the babies.\n\n\nWhy Z-Test/Z Confidence Intervals?\n\nIn many cases, works as well as simulation methods\nEasy to calculate without technology/can do “back of the envelope” analyses\nZ-scores and “estimate \\(\\pm\\) margin of error” are easily interpretable\nClassical methods, used by scientists across disciplines",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference for Proportions (Chapters 16-18)</span>"
    ]
  },
  {
    "objectID": "Section-2-Proportions.html#inference-for-a-comparing-two-proportions-chapter-17",
    "href": "Section-2-Proportions.html#inference-for-a-comparing-two-proportions-chapter-17",
    "title": "2  Inference for Proportions (Chapters 16-18)",
    "section": "2.2 Inference for a Comparing Two Proportions (Chapter 17)",
    "text": "2.2 Inference for a Comparing Two Proportions (Chapter 17)\nWe now move on to consider situations in which two categorical variables are measured on each unit in the sample, and each variable has two possible values. In cases like these, typically one variable is considered the response and one variable is considered explanatory. The explanatory variable may be randomly assigned (like whether or not a subject swam with dolphins) or it may be merely observed (like smoking status). The two possible values of the explanatory variable lead to two groups, and we’re interested in comparing the population proportions that arise from these two groups. We’ll focus on the function of parameters \\(p_1 - p_2\\). The natural estimate of this is \\(\\hat p_1 - \\hat p_2\\): the difference in the sample proportions. We’ll be constructing hypothesis tests to compare \\(p_1\\) to \\(p_2\\) and finding confidence intervals to estimate \\(p_1 - p_2\\).\n\n2.2.1 Randomization tests for the difference in proportions\nExample: Researchers are interested whether electrical brain stimulation will help with problem solving tasks. 40 volunteers were all trained to solve problems in a particular way. Half of the volunteers were randomly assigned to receive electrical stimulation and the other half received a sham stimulation (placebo). All volunteers were then presented with an unfamiliar problem and asked to solve it. The researchers are interested in testing whether the proportion able to solve the problem following electrical stimulation is greater than the proportion able to solve the problem without electrical stimulation.\nThere are a couple of different ways we could state the hypotheses of interest:\n\nH\\(_0:\\)\n\n\n\n\nH\\(_a:\\)\n\n\n\nRecall that hypothesis tests work by assessing how unusual our observed data are, if the null hypothesis is really true. A very unusual result implies that observed data are not likely to have occurred under the null hypothesis. Randomization tests allow us to assess that unusualness by estimating the null distribution–a simulated distribution of what we could expect the distribution of \\(\\hat p_1 - \\hat p_2\\) to look like if H\\(_0\\) is true. We assume H\\(_0\\) is true by recreating the randomization that occurred in the experiment.\nHere are the data:\n\n\n\n\nSolved\nNot Solved\nTotal\n\n\n\n\nSham\n\n\n20\n\n\nElectrical\n\n\n20\n\n\nTotal\n\n\n40\n\n\n\nTo demonstrate what the randomization test is doing, we need 40 cards. Why 40?\n\n\nOf these cards, how many should be red and how many should be black? What do these represent?\n\n\n\nWe’ll shuffle, and deal into two stacks.\n\n\n\n\nA randomization test is going through this shuffling/dealing over and over again, find the difference in proportions for each simulation.\nLet’s look at this in the applets.\nWhat do you notice about the null distribution? How unusual is the observed \\(\\hat p_1 - \\hat p_2\\)?\n\n\n\n\n\n\n\n\nExample: Try Problem 2 in Chapter 17. Set up the hypotheses and describe how a randomization test would work.\n\nHow many cards are needed?\nHow many red? How many black?\nHow many should be dealt into each stack?\nWhat would you calculate from each shuffle/deal?\n\nLet’s do this in R!\n\nFirst, we’ll need to set up the data. This is honestly the hardest part.\n\nVM&lt;-data.frame(Treatment=\"Vaccine\",Response=\"Malaria\",obs=1:89)\nhead(VM)\n\n  Treatment Response obs\n1   Vaccine  Malaria   1\n2   Vaccine  Malaria   2\n3   Vaccine  Malaria   3\n4   Vaccine  Malaria   4\n5   Vaccine  Malaria   5\n6   Vaccine  Malaria   6\n\nVN&lt;-data.frame(Treatment=\"Vaccine\",Response=\"NoMal\",obs=1:203)\nCM&lt;-data.frame(Treatment=\"Control\",Response=\"Malaria\",obs=1:106)\nCN&lt;-data.frame(Treatment=\"Control\",Response=\"NoMal\",obs=1:41)\nmalariadf&lt;-rbind(VM,VN,CM,CN)\nhead(malariadf)\n\n  Treatment Response obs\n1   Vaccine  Malaria   1\n2   Vaccine  Malaria   2\n3   Vaccine  Malaria   3\n4   Vaccine  Malaria   4\n5   Vaccine  Malaria   5\n6   Vaccine  Malaria   6\n\n\nNext, we’ll calculate the observed difference in the proportion of children who contracted malaria between those who received the vaccine and those who received the control. To use the diffmean function, we need to load the mosaic package.\n\nobserved&lt;-diffmean(Response == \"Malaria\" ~ Treatment, data=malariadf)\nobserved\n\n  diffmean \n-0.4162939 \n\n\nNow, we need to shuffle the vaccine/control treatment labels many times and calculate the difference in proportions of malaria for each shuffle. To do this, we’ll need the mosaic library but it’s already been loaded.\n\nmalaria.null&lt;-do(1000)*diffmean(Response == \"Malaria\" ~ shuffle(Treatment),data=malariadf)\nhead(malaria.null)\n\n      diffmean\n1  0.013255987\n2  0.033710745\n3  0.003028609\n4 -0.068563042\n5 -0.048108284\n6 -0.007198770\n\n\nWe’ll plot the null distribution, and to do this we’ll need the ggplot2 library.\n\nlibrary(ggplot2)\n\nggplot(data=malaria.null) + geom_histogram(mapping=aes(x=diffmean)) + \n  xlab(\"Difference in proportions\") +\n  geom_vline(xintercept = observed, linetype=2, color=\"blue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nFinally, we can calculate the p-value by observing how unusual the observed difference in the proportions of malaria is under the null hypothesis.\n\nprop(~diffmean &lt;= observed, data=malaria.null)\n\nprop_TRUE \n        0 \n\n\nWhy are we considering less than or equal to be more extreme in this example?\n\n\n\n\n\n\n2.2.2 Bootstrap confidence interval for the difference in proportions\nAs we saw with a single proportion, bootstrapping will allow us to estimate the variability of \\(\\hat p_1 - \\hat p_2\\) without assuming the null hypothesis is true. With a single proportion, we drew repeated samples (with replacement) from our sample data, and from each bootstrap sample calculated \\(\\hat p_{\\hbox{\\tiny{boot}}}\\). The distribution of the \\(\\hat p_{\\hbox{\\tiny{boot}}}\\) provided an estimation of the sampling distribution of \\(\\hat p\\).\nNow, with two samples, our observed statistic of interest is\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s go to R, and see how this works with the electrical stimulation example. We’ll start by setting up the data and calculating the observed difference in proportion of solved problems between electrical stimulation and control.\n\n#original Sample 1 data (Electrical), creating a data set with 10 S (1) and 10 F (0)#\nelectrical&lt;-c(rep(1,10),rep(0,10))\n\n#original Sample 2 data (Control), creating a data set with 6 S (1) and 14 F (0)#\ncontrol&lt;-c(rep(1,6),rep(0,14))\n\n#Calculate and print the observed statistic, p-hat_E - p-hat_C#\nobs_stat&lt;-mean(electrical)-mean(control)\nobs_stat\n\n[1] 0.2\n\n\nNow, we’ll repeatedly sample with replacement separately from each group. First we need to set up a place to store our summaries from each resample.\n\n#Set up an empty data set with 4 columns: sim number, p_hat_boot_E, p_hat_boot_C, diff#\n\nboot.samples&lt;-data.frame(sim=1:1000,stat_E=NA,stat_C=NA, diff=NA)\n\nhead(boot.samples)\n\n  sim stat_E stat_C diff\n1   1     NA     NA   NA\n2   2     NA     NA   NA\n3   3     NA     NA   NA\n4   4     NA     NA   NA\n5   5     NA     NA   NA\n6   6     NA     NA   NA\n\n#For each row in the data set, draw a bootstrap sample from Sample 1 and Sample 2 and find#\n# p_hat_boot_E and p_hat_boot_C #\n\nfor(i in 1:1000){\n  boot.samples$stat_E[i]&lt;-mean(sample(electrical,size=20,replace=TRUE))\n  boot.samples$stat_C[i]&lt;-mean(sample(control,size=20,replace=TRUE))\n}\n\nhead(boot.samples)\n\n  sim stat_E stat_C diff\n1   1   0.40   0.25   NA\n2   2   0.45   0.20   NA\n3   3   0.60   0.40   NA\n4   4   0.35   0.35   NA\n5   5   0.35   0.15   NA\n6   6   0.50   0.30   NA\n\n#Now find the differences#\n\nboot.samples$diff&lt;-boot.samples$stat_E - boot.samples$stat_C\n\nhead(boot.samples)\n\n  sim stat_E stat_C diff\n1   1   0.40   0.25 0.15\n2   2   0.45   0.20 0.25\n3   3   0.60   0.40 0.20\n4   4   0.35   0.35 0.00\n5   5   0.35   0.15 0.20\n6   6   0.50   0.30 0.20\n\n\nNow let’s plot the bootstrap distribution.\n\nboot.hist&lt;-ggplot(boot.samples, aes(diff)) + geom_histogram(binwidth=0.05)\n\nboot.hist\n\n\n\n\n\n\n\n\nNotice where this distribution is centered!\n\n\n\nWe are re-using this example, since we’ve already carried out the randomization test in the applet. By doing so we are ignoring (maybe) the research question. But, we’re doing both a randomization test and confidence interval so that we can compare the resulting sampling distributions of \\(\\hat p_1 - \\hat p_2\\). What is different? What’s the same?\n\n\n\n\n\n\nNow that we have the bootstrap distribution, we can find the bootstrap percentile confidence interval. Let’s do a 90% interval.\n\n#start by ranking the bootstrap differences from smallest to largest #\nrankdiff&lt;-sort(boot.samples$diff)\n\n#Print out just the first few#\nhead(rankdiff)\n\n[1] -0.35 -0.20 -0.20 -0.20 -0.20 -0.20\n\n#Lower endpoint is the 5th percentile (90% confidence)#\nlower&lt;-rankdiff[50]\nlower\n\n[1] -0.05\n\n#Upper endpoint is the 950th percentile (90% confidence)#\nupper&lt;-rankdiff[950]\nupper\n\n[1] 0.45\n\n\n\n\n\n\n\nBecause our bootstrap distribution is relatively bell-shaped, we could also a calculate a rough 95% boostrap SE confidence interval.\n\nSE&lt;-sd(rankdiff)\nSE\n\n[1] 0.1542495\n\n\n\n\n\n\n\n\n\n\n2.2.3 Mathematical model for the difference in proportions\nFor a single proportion, we needed two conditions to be met to ensure the sampling distribution of \\(\\hat p\\) is approximately normal:\n\n\n\n\n\n\n\nIf these conditions are met, then\n\n\n\n\nWe must meet similar conditions to ensure the sampling distribution of \\(\\hat p_1 - \\hat p_2\\) is approximately normal:\n\n\n\n\n\n\n\n\nIf these conditions are met, then\n\n\n\n\n\n\n\n\n\n\nLike before we don’t know \\(p_1\\) and \\(p_2\\), so we’ll use our best guess. And, like before, our best guess will change depending on whether we’re constructing a confidence interval or carrying out a hypothesis test.\nHow is this going to play out in a hypothesis test?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample (17.5): (Aside: why can’t we do the electrical stimuation example again?). A 2021 Gallup poll surveyed 3941 students pursuing a bachelor’s degree and 2064 students pursuing an associate’s degree. The survey found that 51% of the bachelor’s students (2010) and 44% of the associate’s students (908) said that COVID-19 will negatively impact their ability to complete the degree. We want to decide whether the proportion of bachelor’s students who believe the pandemic will negatively impact degree completion is different from the proportion of associate’s students who believe they will be negatively affected. Let \\(p_B\\) be the proportion of bachelor’s students who believe they’ll be negatively affected and let \\(p_A\\) be the proportion of associate’s students who believe they’ll be negatively affected.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo visualize the p-value:\n\nnormTail(m=0,s=1,L=-5.15, U=5.15)\n\n\n\n\n\n\n\n\nAnd to get the p-value:\n\npnorm(-5.15,mean=0,sd=1)+(1-pnorm(5.15,mean=0,sd=1))\n\n[1] 2.604865e-07\n\n\n\n\n\n\n\n\n\nHow is this going to play out in a confidence interval?\n\n\n\n\n\n\n\n\nExample(17.9): A Kaiser Family Foundation poll for US adults in 2019 found that 79% of Democrats, 55% of Independents, and 24% of Republicans supported a generic “National Health Plan.” There were 347 Democrats, 298 Republicans, and 617 Independents surveyed (Foundation, 2019). We want to estimate the difference between the proportion of Democrats and Independents who support a National Health Plan.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat impacts the width of a confidence interval?\nThere are three main things that impact the width of a confidence interval:\n\nConfidence level\n\n\n\n\nSample size\n\n\n\n\nStandard Error\n\n\n\nNo matter what method we use to calculate the confidence interval, the confidence level is a statement about the long run percentage of confidence intervals that would succeed in capturing the true value of the parameter. What does this mean? Applet\n\n\nHypothesis tests vs. Confidence Intervals\nWhile we should be matching analysis method to the research question, there is a nice relationship between hypothesis tests and confidence intervals. Recall that confidence intervals give a set of plausible values for the unknown parameter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference for Proportions (Chapters 16-18)</span>"
    ]
  },
  {
    "objectID": "Section-2-Proportions.html#inference-for-two-way-tables-chapter-18",
    "href": "Section-2-Proportions.html#inference-for-two-way-tables-chapter-18",
    "title": "2  Inference for Proportions (Chapters 16-18)",
    "section": "2.3 Inference for Two-Way Tables (Chapter 18)",
    "text": "2.3 Inference for Two-Way Tables (Chapter 18)\nSo far, we’ve considered categorical variables with only two possible outcomes: success and failure. Many categorical variables have more than two possible outcomes, so we can’t easily define the proportion of “successes.” Instead, we’ll summarize categorical data with more than two levels using two-way tables. In this class, we’re still going to restrict ourselves to only two variables (often explanatory and response, but not necessarily), both with two or more levels. However, there are certainly statistical methods for more complicated situations.\nTypically, research questions focus on how the proportions of the possible outcomes in the response variable change (or don’t) across the levels of the explanatory variable. However, we can also consider questions about a single variable with more than two outcomes (are the possible outcomes all equally likely? do the possible outcomes follow a particular pattern?) or just whether the two categorical variables are independent or dependent without assigning an explanatory/response relationship. Due to the structure of the variable(s), there really isn’t a population parameter of interest. We can’t (usually) make a function of proportion of successes that makes sense to estimate, like we can with \\(p_1 - p_2\\). That means we’ll be considering only tests, not confidence intervals. We’ll focus on the randomization test and the mathematical model approach. Both methods start with the same set-up.\n\nExample: When surveys are administered, we hope that the respondents give accurate answers. Does the mode of survey delivery affect this? Schober et al (2015) investigated this question. They had 147 people who agreed to be interviewed on an iPhone, and they were randomly assigned to one of three interview modes: human voice, automated voice, text. One question asked was whether they exercise less than once per week during a typical week (a yes is mostly likely considered socially undesirable). The explanatory variable here is survey mode and the response is whether or not the respondent said yes. Here are the data:\n\n\n\n\nText\nHuman Voice\nAutomated Voice\nTotal\n\n\n\n\nExercise Yes\n34\n21\n20\n75\n\n\nExercise No\n124\n139\n139\n402\n\n\nTotal\n158\n160\n159\n477\n\n\n\nBased on these data, it looks like the answer to the question does change depending on survey mode, with respondents more likely to say yes via text. However, we don’t know if this result could have happened by chance.\n\n\n\n\n\n\n\n\n\n\n2.3.1 Expected Counts\nWe don’t expect the proportion of `yes’ to be exactly the same across all survey modes, but we want to know if these vary enough to convince us that survey mode and answer are not independent. To do this, we need to find for each cell in the table.\n\nAgain, here are the observed data\n\n\n\n\nText\nHuman Voice\nAutomated Voice\nTotal\n\n\n\n\nExercise Yes\n34\n21\n20\n75\n\n\nExercise No\n124\n139\n139\n402\n\n\nTotal\n158\n160\n159\n477\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText\nHuman Voice\nAuto Voice\nTotal\n\n\n\n\nExercise Yes\n34 (_____)\n21 (_____)\n20 (_____)\n75\n\n\nExercise No\n124 (_____)\n139 (_____)\n139 (_____)\n402\n\n\nTotal\n158\n160\n159\n477\n\n\n\n\nSo now the key question…are the observed and expected cell counts different enough?\n\nCell(1,1) obs - exp = 34 -\nCell(1,2) obs - exp = 21 -\nCell(1,3) obs - exp = 20 -\nCell(2,1) obs - exp = 124 -\nCell(2,2) obs - exp = 139 -\nCell(2,3) obs - exp = 139 -\n\n\n\n\n\n\nNew test statistic!\n\n\n\n\n\n\nIn our example:\n\nCell(1,1) (obs - exp)\\(^2\\)/exp = \\((34 - 24.84)^2/(24.84) = 9.16^2/24.84 = 3.3778\\)\nCell(1,2) (obs - exp)\\(^2\\)/exp = \\((21 - 25.16)^2/(25.16) = (-4.16)^2/25.16 = 0.6878\\)\nCell(1,3) (obs - exp)\\(^2\\)/exp = \\((20 - 25)^2/(25) = (-5)^2/25 = 1\\)\nCell(2,1) (obs - exp)\\(^2\\)/exp = \\((124 - 133.16)^2/(133.16) = (-9.16)^2/133.16 = 0.6301\\)\nCell(2,2) (obs - exp)\\(^2\\)/exp = \\((139 - 134.84)^2/(134.84) = 4.16^2/134.84 = 0.1283\\)\nCell(2,3) (obs - exp)\\(^2\\)/exp = \\((139 - 134)^2/(134) = 5^2/134 =0.3731\\)\n\n\n\n\nTo see if this is ‘big’ we need the sampling distribution of our new test statistic. We can estimate that sampling distribution using either a randomization test or the mathematical model approach.\n\n\n\n2.3.2 Randomization Test\nThe randomization test for a two-way table works just like it does with two samples. We’ll randomize by shuffling and dealing/assigning the 75 yes answers and 402 no answers to the three survey modes at random.\n\nHow many colors of cards?\n\n\n\n\n\nHow many stacks to deal them into? How many in each stack?\n\n\n\n\n\nWhat do we find for each deal/shuffle?\n\n\n\n\nApplet\n\n\n\n\n\n\n\nConclusion:\n\n\n\n2.3.3 Mathematical Model\nBased on what we just observed in the applet, the normal distribution is not going to be a good approximation to sampling distribution. It turns out this test statistic follows a different mathematical distribution, the chi-squared distribution (proof: see STAT 462). The normal distribution has two parameters that determine its shape: the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). The shape of the chi-square distribution is determined by a parameter called the . Figure 18.2 on page 307 shows how the shape of the distribution changes depending on the df.\nSo how can we use this?\n\n\n\n\n\n\n\n\n\nAgain, we have conditions that need to be met for the mathematical model to be a good approximation:\n\n\n\n\n\n\nExample: Let’s go back and do the survey mode example using the mathematical model approach. First, we’ll need to check the conditions are met:\n\n\n\n\n\n\n\n\nTo find the p-value, we can use the R function pchisq(). Like pnorm it gives area to the left. So,\n\npchisq(6.1971,df=2,lower.tail=FALSE)\n\n[1] 0.04511457\n\n\n\nWe can also do this directly in R.\n\nsurveymode&lt;-read.csv(\"surveymode.csv\",header=TRUE)\n\n#make a table#\nmode&lt;-table(surveymode$Response,surveymode$Mode)\n\n#see the table, note alphabetical order#\nmode\n\n     \n      Avoice Hvoice Text\n  No     139    139  124\n  Yes     20     21   34\n\n#Chi-square mathematical model test#\nchisq.test(mode)\n\n\n    Pearson's Chi-squared test\n\ndata:  mode\nX-squared = 6.0069, df = 2, p-value = 0.04962\n\n#Chi-square randomization test#\nchisq.test(mode,simulate.p.value=TRUE, B=1000)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 1000\n    replicates)\n\ndata:  mode\nX-squared = 6.0069, df = NA, p-value = 0.05095",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inference for Proportions (Chapters 16-18)</span>"
    ]
  },
  {
    "objectID": "Section-3-Means.html",
    "href": "Section-3-Means.html",
    "title": "3  Inference for Means (Chapters 19-22)",
    "section": "",
    "text": "3.1 Inference for a Single Mean (Chapter 19)\nSo far, we’ve discussed randomization, bootstrap, and mathematical models as methods to approximate/describe a sampling distribution and quantify variability, as well as how these methods can be used to answer research questions about categorical data. Now, we turn to how these three methods can be used to answer research questions for quantitative data, specifically a quantitative response. We’ll consider scenarios where there is a single numerical variable measured (one mean, Chapter 19), scenarios where a categorical explanatory variable with two possible values is recorded/assigned and a numerical response variable is observed (two independent means, Chapter 20), and scenarios in which a categorical explanatory variable with more than two possible values is recorded/assigned and a numerical response variable is observed (many means, Chapter 22). We’ll also encounter a new scenario: the difference between paired observations (Chapter 21). We’ll also meet two new distributions!\nIn all of these scenarios the parameter(s) of interest is the mean (\\(\\mu\\)) of the population(s) under consideration. The natural estimator of the population mean is the sample mean, \\(\\bar X\\). In Chapter 19 (and, spoiler alert, Chapter 21) we’ll have a single \\(\\mu\\). In Chapter 20 we’ll have two \\(\\mu_i\\)s, and in Chapter 22 we’ll have several \\(\\mu_i\\)s.\nLike we did with proportions, we’ll rely on the Central Limit Theorem to model \\(\\bar X\\) using the normal distribution when we consider the mathematical model approaches. Also as with proportions, certain conditions must be met for this approach to be valid. We’ll discuss those conditions in each of the data scenarios. We’ll start with a single variable measured on each sample unit, where the observation results in a number.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference for Means (Chapters 19-22)</span>"
    ]
  },
  {
    "objectID": "Section-3-Means.html#inference-for-a-single-mean-chapter-19",
    "href": "Section-3-Means.html#inference-for-a-single-mean-chapter-19",
    "title": "3  Inference for Means (Chapters 19-22)",
    "section": "",
    "text": "3.1.1 Bootstrap Confidence Intervals for a Mean\nConsider the following scenario. We’d like to learn about the true average wait time at Starbucks for a particular drink. To learn about this, we go to 6 randomly selected Starbucks locations in the same city, all at 10:00 am on Monday. At each location we order the same drink and observe the waiting time in seconds until it is prepared. The parameter of interest is\n\\(\\mu =\\)\nThe sample statistic is\n\\(\\bar X =\\)\nSuppose we observed wait times of: 110, 54, 76, 123, 91, and 101. Based on our sample of six locations, the sample average wait time is \\(\\bar x = 92.5\\) seconds with sample standard deviation \\(s=24.76\\) seconds.\n\n\n\n\n\n\n\n\nLike we did with proportions, we can use the bootstrap method to approximate the variability we expect to see in sample means (calculated from 6 observations) from sample to sample:\n\n\n\n\n\nLet’s go to R! We’ll start by setting up the data\n\nwaittime&lt;-c(110, 54, 76, 123, 91, 101)\nwaittime\n\n[1] 110  54  76 123  91 101\n\n\nNow, we’ll find the observed sample mean and standard deviation from our 6 observations:\n\nmean(waittime) \n\n[1] 92.5\n\nsd(waittime)\n\n[1] 24.76086\n\n\nWe’ll draw bootstrap samples just like we did with proportions, draw repeated samples of size 6 from our data.\n\nlibrary(ggplot2)\n\n#Set up an empty data set with 2 columns: simulation number, bootstrap mean#\n\nboot.samples&lt;-data.frame(sim=1:1000,mean_WT=NA)\n\n#For each row in the data set, draw a bootstrap sample from the original data and find#\n# mean_WT#\n\nfor(i in 1:1000){\n  boot.samples$mean_WT[i]&lt;-mean(sample(waittime,size=6,replace=TRUE))\n}\n\n#Histogram#\nboot.hist&lt;-ggplot(boot.samples, aes(mean_WT)) + geom_histogram(binwidth=5)\n\n#See the plot#\nboot.hist\n\n\n\n\n\n\n\n#To get the bootstrap percentile confidence interval, #\n#start by ranking the bootstrap means from smallest to largest #\nrankmean&lt;-sort(boot.samples$mean_WT)\n\n#Lower endpoint is the 2.5th percentile (95% confidence)#\nlower&lt;-rankmean[25]\nlower\n\n[1] 73.16667\n\n#Upper endpoint is the 97.5th percentile (95% confidence)#\nupper&lt;-rankmean[975]\nupper\n\n[1] 109.1667\n\n\nThis will give us a bootstrap percentile confidence interval.\n\n\n\n\nThe histogram of the bootstrapped sample means is relatively bell-shape, so we could also find a bootstrap SE confidence interval. For that, we’ll need the bootstrap SE (the standard deviation of the bootstrapped sample means).\n\n#Bootstrap standard error of the mean#\nsd(rankmean)\n\n[1] 9.321514\n\n\n\n\n\n\n\n\nThe bootstrap method works for other statistics as well (even when the mathematical model does not)–like standard deviation, median, range, etc. With other stats we won’t necessarily end up a bell-shaped distribution. That’s okay–we can use the percentile method.\nFor example, we could use the bootstrap approach to get a confidence interval for \\(\\sigma\\), the true standard deviation of wait time.\n\n#Set up an empty data set with 2 columns: simulation number, bootstrap SD#\nboot.samples&lt;-data.frame(sim=1:1000, sd_WT=NA)\n\n#For each row in the data set, draw a bootstrap sample from the original data and find#\n# sd_WT#\n\nfor(i in 1:1000){\n  boot.samples$sd_WT[i]&lt;-sd(sample(waittime,size=6,replace=TRUE))\n}\n\n#Histogram#\nboot.hist&lt;-ggplot(boot.samples, aes(sd_WT)) + geom_histogram(binwidth=2)\n\n#See the plot#\nboot.hist\n\n\n\n\n\n\n\n#To get the bootstrap percentile confidence interval, #\n#start by ranking the bootstrap sds from smallest to largest #\nranksd&lt;-sort(boot.samples$sd_WT)\n\n#Lower endpoint is the 5th percentile (90% confidence)#\nlower&lt;-ranksd[50]\nlower\n\n[1] 10.77806\n\n#Upper endpont is the 95th percentile (90% confidence)#\nupper&lt;-ranksd[950]\nupper\n\n[1] 31.32677\n\n\n\n\n\n\nExample: Wildlife researchers trapped and measured six adult male collared lemmings. The data (in mm) are: 104, 99, 112, 115, 96, 109. Use bootstrap methods to find a 95% confidence interval for the true mean size of adult male collared lemmings. Use both the percentile approach and the bootstrap SE approach.\n\n\n\n\n\n\n\n\n\n3.1.2 Mathematical Model Approach for a Mean\nLike with proportions, we’ll use the Central Limit Theorem here.\n\n\n\n\n\n\nThis presents a few complications:\n\n\n\n\n\n\n\nThe natural fix is to use \\(s\\) (the sample standard deviation) in place of \\(\\sigma\\), so \\(SE=\\)\nBut this leads to yet another complication: the normal distribution isn’t quite right. We end up with a distribution that has heavier tails than the normal.\nInstead, we use the \\(t\\)-distribution which, like the chi-squared, has the degrees of freedom parameter:\n\ndegrees of freedom determines the shape of the \\(t\\), with the distribution getting closer and closer to the normal as the \\(df\\) increase\nDemo: Compare t and Z\nAs \\(df \\rightarrow \\infty\\), the \\(t\\) goes to the standard normal.\nIn this scenario of a single mean, \\(df=\\)\nR function: pt(q,df)\n\n\n3.1.2.1 Mathematical model confidence intervals for a single mean\nLet’s work through confidence intervals for a single mean by way of example. Suppose we want to get a sense of the average number of goals scored per game in the NHL, and the average margin of victory. We record data on all 44 NHL games played over a Thursday-Monday in December. Let’s first look at the number of goals. The data are in Canvas.\nWe’ll start with visualizing the data in R\n\n#Read in the NHL data#\nhockey&lt;-read.csv(\"NHLGames.csv\",header=TRUE)\nhead(hockey)\n\n  Goals MarginVictory\n1     6             2\n2     3             1\n3     9             1\n4     7             3\n5     6             2\n6     5             1\n\nlibrary(ggplot2)\n\n#Summarize Number of Goals#\nsummary(hockey$Goals)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   5.000   6.000   6.114   7.000   9.000 \n\nsGoals&lt;-sd(hockey$Goals)\nsGoals\n\n[1] 1.728232\n\nGoals.dot&lt;-ggplot(hockey, aes(hockey$Goals)) + geom_dotplot()\nGoals.dot\n\nWarning: Use of `hockey$Goals` is discouraged.\nℹ Use `Goals` instead.\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nAre the conditions for the mathematical model met?\n\nSample size\nIndependence\n\n\n\n\n\nThe general form of the confidence interval hasn’t changed:\n\\[\n\\hbox{point estimate} \\pm \\hbox{multiplier} \\times SE\n\\]\n\n\nIn our data set set, there are \\(n=44\\) games.\n\n#For a confidence interval, need the multiplier for a 90% confidence interval, puts 0.05 in left tail#\nqt(0.05, df=43, lower.tail=FALSE)\n\n[1] 1.681071\n\n\n\n\n\n\n\n\n\n\n\nWhat about a 99% confidence interval? What would change?\n\n\n\n\nWhat about a 95% confidence interval for margin of victory?\n\n#Summarize Margin of Victory#\nsummary(hockey$MarginVictory)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   2.000   2.159   3.000   6.000 \n\nMargin.dot&lt;-ggplot(hockey, aes(hockey$MarginVictory)) + geom_dotplot()\nMargin.dot\n\nWarning: Use of `hockey$MarginVictory` is discouraged.\nℹ Use `MarginVictory` instead.\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\nsMV&lt;-sd(hockey$MarginVictory)\nsMV\n\n[1] 1.328457\n\n\n\n\n\n\n\n\n\n\n\n3.1.2.2 Mathematical model hypothesis tests for a single mean\nJust as with confidence intervals, the form of the test statistic doesn’t (typically) change as we move from data type to data type:\n\\[\n\\hbox{test statistic} = \\frac{\\hbox{observed value - null value}}{SE}\n\\] So now,\n\n\n\n\nIf the null hypothesis is true and the conditions are met, then our test statistic follows a \\(t-\\)distribution with \\(df = n-1\\). The conditions are the same: independent observations and a large enough sample size with no extreme outliers. We can use the R function pt(T,df=) to get p-values.\nExample: The Lincoln Marathon is the 51st largest marathon in the US, and is a qualifier for the Boston Marathon. From 2003 to 2019, the average finish time was 253.25 minutes (4 hours and 13 minutes, 15 seconds). The race was not run in 2020. We want to see if the break changed the average finish time. We took a random sample of 50 finishers from the 2021 race. For this random sample of 50, \\(\\bar x = 261.38\\) minutes and \\(s=51.87\\) minutes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Consider a manufacturing process for hypodermic needles used for blood donation. The needles need to have a diameter of 1.65 mm. If the needles are too big, they hurt the donor. Too small, and they’ll rupture the red blood cells, making the donated blood useless. During every shift, quality control staff take a random sample of several needles and measure their diameter. If there’s a problem, they shut down the manufacturing process to correct it. Suppose the most recent sample of 35 needles had an average diameter of 1.64 mm and a standard deviation of 0.07 mm. Suppose the diameters of needles have a bell-shaped distribution. Based on these data, should the process be shut down?\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: The General Social Survey (GSS) is a survey of a representative sample of U.S. adults who are not institutionalized. A 2018 General Social Survey asked a random sample of 1,118 adults how often they contacted their closest friend by either phone, internet, other communication device, or face-to-face. Of the 1,118 responses, the average number of times per week the respondents contacted their closest friend was 2.87, with a standard deviation of 2.46. The sample data are not strongly skewed. We want to estimate the mean number of closest friend contacts per week.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside: How do we know from the sample mean and standard deviation that the distribution of contact times cannot be bell-shaped? Why is it still okay to use the mathematical model?\n\nWe can also do these in R using t.test, but we’ll need the full data set, not just the summary statistics.\nFor the NHL data,\n\nt.test(hockey$Goals, mu=5, alternative=\"greater\")\n\n\n    One Sample t-test\n\ndata:  hockey$Goals\nt = 4.2743, df = 43, p-value = 5.221e-05\nalternative hypothesis: true mean is greater than 5\n95 percent confidence interval:\n 5.675649      Inf\nsample estimates:\nmean of x \n 6.113636",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference for Means (Chapters 19-22)</span>"
    ]
  },
  {
    "objectID": "Section-3-Means.html#inference-for-a-two-independent-means-chapter-20",
    "href": "Section-3-Means.html#inference-for-a-two-independent-means-chapter-20",
    "title": "3  Inference for Means (Chapters 19-22)",
    "section": "3.2 Inference for a Two Independent Means (Chapter 20)",
    "text": "3.2 Inference for a Two Independent Means (Chapter 20)\nNow, we’ll extend the methods for a single mean to differences in population means that come from two groups. So, we’ll now focus on constructing hypothesis tests about and estimating the function of parameters \\(\\mu_1 - \\mu_2\\), where \\(\\mu_1\\) is the mean of Group 1 an \\(\\mu_2\\) is the mean of Group 2. A reasonable point estimate is \\(\\bar x_1 - \\bar x_2\\), the difference in sample means.\nAs we did with two proportions, we’ll look at analysis three different ways: randomization test; bootstrap to find an interval estimate; mathematical framework for tests and confidence intervals (assuming the conditions are met to use a normal approximation. One note: one of the conditions for these techniques (no matter which) is the groups are independent. What happens in Group 1 has no bearing on Group 2. If there is any dependence among the groups (twin studies, before-and-after studies, for example) these are not appropriate. This was not really a concern with proportions, but can occur quite naturally with means. We’ll consider dependence between the groups in a future section.\n\n3.2.1 Randomization test for the difference in means\nWhen we were working with proportions, we carried out a randomization test using two colors of cards. One color represented success, and the other color represented failure. We shuffled the cards, and dealt them into two stacks, representing our two groups. We then found the proportion of successes in each stack, and took the difference in proportions. We then did this shuffling/dealing many times. We’ll see through an example how our process changes when dealing with quantitative data.\n\nExample: The research question we are interested in investigating is whether playing violent video games lead people to more or less aggressive behavior. Hollingdale and Greitemeyer (2014) approached the question in this way. They randomly assigned 49 students from a UK university to play Call of Duty: Modern Warfare (violent) and 52 students to play LittleBigPlanet (not violent). After 30 minutes playing the video games, the subjects were asked to complete a marketing survey investigating a new hot chili sauce recipe. They were told to prepare some chili sauce for a taste tester and that the taste tester “couldn’t stand hot chili sauce but were taking part due to good payment.” They were then presented with that appeared to be a very hot chili sauce and asked to spoon what they thought would be an appropriate amount into a bowl for a new recipe. The amount of chili sauce was weighed in grams after the participant left the experiment. The amount of sauce was used as a measure of aggression: the more chili sauce, the greater the subject’s aggression.\n\nIs this an experiment or an observational study? How do you know?\n\n\n\n\n\nHow do we know this is involves quantitative data?\n\n\n\n\nParameters:\n\n\n\n\n\n\nHypotheses:\n\n\n\n\n\nThe resulting data are:\n\n\n\nGroup\n\\(n\\)\nMean\nSD\nMin\nMax\n\n\n\n\nViolent\n49\n16.12\n15.30\n1\n63\n\n\nNonviolent\n52\n9.06\n7.65\n0\n38\n\n\n\nSo our observed statistic is:\n\nOur goal is the same as it was with proportions: to determine whether the observed difference in sample means is likely to have occurred by chance if the null hypothesis is really true.\nJust like we shuffled cards in the two-proportions case, we’re going to have cards again. Like before, the shuffling implements the null hypothesis model–there is no effect of the violent video game. The amount of chili sauce selected doesn’t depend on whether or not the participant just played a violent game. We’d sometimes expect participants to use slightly more chili sauce if they’d just played a violent game (\\(\\bar x_{\\tiny{\\hbox{violent}}} &gt; \\bar x_{\\tiny{\\hbox{nonviolent}}}\\) and sometimes expect participants to use slightly less chili sauce if they’d just played a violent video game (\\(\\bar x_{\\tiny{\\hbox{violent}}} &lt; \\bar x_{\\tiny{\\hbox{nonviolent}}}\\) just due to natural variability.\nBefore, we looked at red and black cards, shuffled, and dealt into two stacks representing our two groups. Now, color isn’t enough. Instead, we’ll still have \\(n_{total} = 49 + 52\\) total cards, but we’ll write on the cards the observed amount of chili sauce. Then shuffle, and deal into stacks. One stack will get 49 cards (representing the 49 violent players) and the other will get 52 cards (representing the 52 nonviolent players). We’ll find the difference in sample means after the shuffling/dealing. We’ll repeat this process many times, and look to see how unusual our observed \\(\\bar x_{\\hbox{\\tiny{violent}}} - \\bar x_{\\hbox{\\tiny{nonviolent}}} = 7.065\\) is.\n\nIn the applet\n\n\n\n\n\nIn R\n\nWe’ll start by loading the necessary packages and reading in the data.\n\nlibrary(ggplot2)\nlibrary(mosaic)\n\n\nchili&lt;-read.csv(\"chili.csv\",header=TRUE)\n\nhead(chili)\n\n  VideoGame ChiliSauce\n1   violent         42\n2   violent          4\n3   violent         27\n4   violent          2\n5   violent         10\n6   violent          5\n\n#Get the sample mean for each group#\nmean(ChiliSauce~VideoGame, data=chili)\n\nnonviolent    violent \n  9.057692  16.122449 \n\n#Find the difference in sample means#\nobs_diff &lt;- diff(mean(ChiliSauce~VideoGame, data=chili))\nobs_diff\n\n violent \n7.064757 \n\n#Construct box plots for each group#\n\ngf_boxplot(ChiliSauce~VideoGame,data=chili)\n\n\n\n\n\n\n\n#Scramble the treatment groups with respect to outcome many times to get the null distribution#\n\nnull_dist &lt;- do(1000)*diff(mean(ChiliSauce~shuffle(VideoGame), data=chili))\n\nhead(null_dist)\n\n    violent\n1 -2.012559\n2 -1.913462\n3 -3.915228\n4 -4.291797\n5 -3.281005\n6  0.405416\n\n#Histogram of the null distribution#\nggplot(data=null_dist) + geom_histogram(mapping=aes(x=violent))+ \n  xlab(\"Difference in means\") +\n  geom_vline(xintercept = obs_diff, linetype=2, color=\"blue\") +\n  geom_vline(xintercept = -obs_diff, linetype=2, color=\"blue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Calculate the proportion of simulated differences in mean as or more extreme than obs#\nprop(~violent &gt;= obs_diff, data=null_dist)+prop(~violent &lt;= -obs_diff, data=null_dist)\n\nprop_TRUE \n    0.003 \n\n#Another way to visualize!#\ngf_histogram(~violent, fill = ~(abs(violent) &gt;= obs_diff), data=null_dist,\n             binwidth=0.5,xlab=\"Distribution of difference in means under the null hypothesis\")\n\n\n\n\n\n\n\n\n\n\n3.2.2 Bootstrap confidence interval for the difference in means\nWhen we used bootstrapping to find confidence intervals for the difference in two proportions, we took a bootstrap sample separately from each group and calculated the difference in the resulting proportions. We’re going to do the same thing here–take a bootstrap sample from each group, find the two sample bootstrap means, and then find the difference in the bootstrap means. Doing this over and over again will allow us to explore the variability/sampling distribution of the difference in sample means.\nExample: A random sample of college baseball players and a random sample of (male) college soccer players were obtained independently and weighed. The table below shows the weights (in pounds) (also a .csv file in Canvas).\n\n\n\nBaseball\nSoccer\nBaseball\nSoccer\n\n\n\n\n190\n165\n186\n156\n\n\n200\n190\n210\n168\n\n\n187\n185\n198\n173\n\n\n182\n187\n180\n158\n\n\n192\n183\n182\n150\n\n\n205\n189\n193\n172\n\n\n185\n170\n200\n180\n\n\n177\n182\n195\n184\n\n\n\nWe are interested in estimating the differences in mean weight between baseball players and soccer players.\nLet’s look at R, and read in the data.\n\nathletes&lt;-read.csv(\"Athletes.csv\",header=TRUE)\n\nhead(athletes)\n\n  Baseball Soccer\n1      190    165\n2      200    190\n3      187    185\n4      182    187\n5      192    183\n6      205    189\n\n\n\n#observed mean and sd#\nbasemean&lt;-mean(athletes$Baseball) \nbasemean\n\n[1] 191.375\n\nbasesd&lt;-sd(athletes$Baseball) \nbasesd\n\n[1] 9.464847\n\nsoccermean&lt;-mean(athletes$Soccer) \nsoccermean\n\n[1] 174.5\n\nsoccersd&lt;-sd(athletes$Soccer) \nsoccersd\n\n[1] 12.49533\n\n#Set up an empty data set with 4 columns: simulation number, \n# bootstrap mean for baseball, bootstrap mean for soccer, difference#\n\nboot.samples&lt;-data.frame(sim=1:1000,mean_base=NA,mean_soccer=NA,diff=NA)\n\n#For each row in the data set, draw a bootstrap sample from the original data and find#\n# mean_base and mean_soccer#\n\nfor(i in 1:1000){\n  boot.samples$mean_base[i]&lt;-mean(sample(athletes$Baseball,size=16,replace=TRUE))\n  boot.samples$mean_soccer[i]&lt;-mean(sample(athletes$Soccer,size=16,replace=TRUE))\n  boot.samples$diff[i]&lt;-boot.samples$mean_base[i]-boot.samples$mean_soccer[i]\n}\n\nhead(boot.samples)\n\n  sim mean_base mean_soccer    diff\n1   1  192.1250    174.2500 17.8750\n2   2  190.5000    172.2500 18.2500\n3   3  187.0625    174.0000 13.0625\n4   4  194.6250    170.8750 23.7500\n5   5  191.8750    180.1875 11.6875\n6   6  193.6875    175.1875 18.5000\n\n#Histogram#\nboot.hist&lt;-ggplot(boot.samples, aes(diff)) + geom_histogram(binwidth=2)\n\n#See the plot#\nboot.hist\n\n\n\n\n\n\n\n#To get the bootstrap percentile confidence interval, #\n#start by ranking the bootstrap means from smallest to largest #\nrankmean&lt;-sort(boot.samples$diff)\n\n#Lower endpoint is the 2.5th percentile (95% confidence)#\nlower&lt;-rankmean[25]\nlower\n\n[1] 9.6875\n\n#Upper endpont is the 97.5th percentile (95% confidence)#\nupper&lt;-rankmean[975]\nupper\n\n[1] 23.875\n\n#Bootstrap standard error of the mean#\nsd(rankmean)\n\n[1] 3.745269\n\n\nSo we can get a bootstrap percentile interval or a bootstrap SE interval.\n\n\n\n\n\n\nPractice: (from book, page 346-348) Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack? The data are in the library . Try to find a 95% confidence interval for \\(\\mu_{ESC} - \\mu_{Control}\\) using the bootstrap percentile confidence interval and the bootstrap SE confidence interval.\n\n\nMore Practice: The Canvas file ‘shoes.csv’ contains data on many styles of athletic shoes. Find a confidence interval for the difference in mean price for road versus trail shoes.\n\n\n\n3.2.3 Mathematical model for the difference in means\nJust like with mathematical model methods for single means, we need to check conditions to determine whether we can the \\(t\\)-distribution to construct tests and form confidence intervals for the difference in means.\n\nIndependence–both between and within groups\nCheck normality of each group separately (basically checking for extreme outliers)\nIf these are both met, then the standard error of \\(\\bar x_1 - \\bar x_2\\) is \\(SE = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma^2_2}{n_2}}\\) with \\(df =\\) really complicated (you’ll see we get non-integers in R–it’s doing the complicated calculation). We’ll use \\(\\min(n_1-1, n_2-1)\\) if we’re not using R. We won’t know \\(\\sigma^2_1\\) and \\(\\sigma_2^2\\), so we’ll approximate the standard error using \\(SE \\approx \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}}\\)\n\nAs with tests for a single mean (and one proportion, and two proportions), our test statistic will have the usual form: \\[\n\\hbox{test statistic} = \\frac{\\hbox{observed value - hypothesized value}}{SE}\n\\] In the case of two means, this is \\[\nT = \\frac{(\\bar x_1 - \\bar x_2) - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\] When the null hypothesis is true and the conditions are met, \\(T\\) has a \\(t\\)-distribution with \\(df=\\min(n_1-1,n_2-1)\\).\nConfidence intervals will also have the same form: \\[\n\\hbox{observed statistic} \\pm \\hbox{multiplier} \\times SE\n\\] For this specific situation of comparing two independent means, this is \\[\n(\\bar x_1 - \\bar x_2) \\pm t^*_{df} \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s^2_2}{n_2}}\n\\] and we’ll again use \\(df=\\min(n_1-1,n_2-1)\\) (or let R calculate it for us).\nWith two proportions, our SE depending on whether we were doing a hypothesis test or calculating a confidence interval. Here, it doesn’t. Any guesses why?\n\nExample: The data set SleepStudy contains data on 253 students who did skills tests to measure cognitive function, completed a survey about attitude and habits, and kept a sleep diary. The data were reported in Onyper, et al. (2012). There are lots of different potential variables to consider. The data set includes:\n\nGender: 1=male, 0=female\nClassYear: Year in school, 1=first year, \\(\\dots\\), 4=senior\nLarkOwl: Early riser or night owl? Lark, Neither, or Owl\nNumEarlyClass: Number of classes per week before 9 am\nEarlyClass: Indicator for any early classes\nGPA: Grade point average (0-4 scale)\nClassesMissed: Number of classes missed in a semester\nCognitionZScore: Z-score on a test of cognitive skills\nPoorSleepQuality: Measure of sleep quality (higher values are poorer sleep)\nDepressionScore: Measure of degree of depression\nAnxietyScore: Measure of amount of anxiety\nStressScore: Measure of amount of stress\nDepressionStatus: Coded depression score: normal, moderate, or severe\nAnxietyStatus: Coded anxiety score: normal, moderate, or severe\nStress: Coded stress score: normal or high\nDASScore: Combined score for depression, anxiety, and stress\nHappiness: Measure of degree of happiness\nAlcoholUse: Self-reported: Abstain, light, moderate, or heavy\nDrinks: Number of alcoholic drinks per week\nWeekdayBed: Average weekday bedtime (24.0 = midnight)\nWeekdayRise: Average weekday rise time (8.0 = 8 am)\nWeekdaySleep: Average hours of sleep on weekdays\nWeekendBed: Average weekend bedtime (24.0 = midnight)\nWeekendRise: Average weekend rise time (8.0 = 8 am)\nWeekendSleep: Average hours of sleep on weekends\nAverageSleep: Average hours of sleep for all days\nAllNighter: Had an all-nighter this semester? 1=yes, 0=no\n\nLet’s consider the variables GPA and stress (coded normal or high). We’d like to know if there is convincing evidence that those with high stress levels have a different mean GPA than those with normal stress levels.\n\nHypotheses:\n\n\n\nCheck conditions:\n\nIndependent observations?\nLarge enough sample sizes?\n\n\n\\[\nT = \\frac{(\\bar x_1 - \\bar x_2) - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} = \\hspace{90mm}\n\\]\n\nsleep&lt;-read.csv(\"SleepStudy.csv\",header=TRUE)\n\n#Those with normal stress#\nsummary(sleep$GPA[sleep$Stress==\"normal\"])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   3.000   3.250   3.209   3.500   4.000 \n\nsum(with(sleep,Stress==\"normal\"))\n\n[1] 197\n\nsd(sleep$GPA[sleep$Stress==\"normal\"])\n\n[1] 0.412318\n\n#Those with high stress#\nsummary(sleep$GPA[sleep$Stress==\"high\"])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.500   3.065   3.350   3.366   3.603   4.000 \n\nsum(with(sleep,Stress==\"high\"))\n\n[1] 56\n\nsd(sleep$GPA[sleep$Stress==\"high\"])\n\n[1] 0.3513277\n\ndotplot&lt;-ggplot(sleep, aes(GPA)) + geom_dotplot(aes(color=Stress)) + facet_grid(~Stress)\n\ndotplot\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd we can get the p-value:\n\n2*pt(-2.835,df=55)\n\n[1] 0.00639864\n\n\n\nIn R, we can carry out this test very easily:\n\nt.test(GPA~Stress,data=sleep)\n\n\n    Welch Two Sample t-test\n\ndata:  GPA by Stress\nt = 2.8397, df = 102.11, p-value = 0.005451\nalternative hypothesis: true difference in means between group high and group normal is not equal to 0\n95 percent confidence interval:\n 0.04741781 0.26711265\nsample estimates:\n  mean in group high mean in group normal \n            3.366250             3.208985 \n\n\n\n\n\nWe can also use the subset function to restrict our attention to a specific value of another variable. For example, suppose we want to determine whether average hours of weekday sleep (WeekdaySleep) differs between those who have at least one early class and those who do not (EarlyClass; 0=no; 1=yes). However, we want to restrict our attention only to those who consider themselves Night Owls (LarkOwl=Owl). We could do this with:\n\nsleep2&lt;-subset(sleep,LarkOwl=='Owl')\n\nhead(sleep2)\n\n   Gender ClassYear LarkOwl NumEarlyClass EarlyClass  GPA ClassesMissed\n3       0         4     Owl             0          0 2.97            12\n5       0         4     Owl             0          0 3.20             4\n20      1         3     Owl             3          1 2.80            20\n26      1         2     Owl             0          0 3.07            20\n27      1         2     Owl             4          1 3.00            10\n46      1         3     Owl             0          0 3.50             2\n   CognitionZscore PoorSleepQuality DepressionScore AnxietyScore StressScore\n3             0.38               18              18           18           9\n5             1.22                9               7           25          14\n20           -0.57               15               6            5          10\n26           -0.62                9              23            5          16\n27            0.16                5              14           13          14\n46            0.47                6               9            5          19\n   DepressionStatus AnxietyStatus Stress DASScore Happiness AlcoholUse Drinks\n3          moderate        severe normal       45        17      Light      3\n5            normal        severe normal       46        15   Moderate      4\n20           normal        normal normal       21        19   Moderate      7\n26           severe        normal   high       44        24    Abstain      0\n27         moderate      moderate normal       41        28   Moderate      8\n46           normal        normal   high       33        24   Moderate      9\n   WeekdayBed WeekdayRise WeekdaySleep WeekendBed WeekendRise WeekendSleep\n3       27.44        6.55         3.00      28.00       12.59        10.09\n5       25.90        8.67         6.09      23.75        9.50         7.00\n20      25.25        8.12         7.20      25.67       10.50         8.00\n26      24.70       11.02        10.32      27.50       12.25         8.75\n27      24.40        7.95         8.45      24.00       11.75         9.50\n46      24.95        8.86         7.97      27.00       11.63         9.13\n   AverageSleep AllNighter\n3          5.02          0\n5          6.35          0\n20         7.43          0\n26         9.87          0\n27         8.75          0\n46         8.30          0\n\nt.test(WeekdaySleep~EarlyClass, data=sleep2)\n\n\n    Welch Two Sample t-test\n\ndata:  WeekdaySleep by EarlyClass\nt = 0.79051, df = 30.009, p-value = 0.4354\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.5768755  1.3055071\nsample estimates:\nmean in group 0 mean in group 1 \n       7.986316        7.622000 \n\n\n\nFor confidence intervals, let’s go back to the baseball and soccer players. The summary statistics are:\n\n\n\nGroup\n\\(n\\)\nMean\nSD\n\n\n\n\nBaseball\n16\n191.375\n9.465\n\n\nSoccer\n16\n174.5\n12.495\n\n\n\nWe do need to check the conditions to see if we can use the mathematical model. We can use dotplot to check for extreme outliers. However, this data set is structured differently than we have seen before, and we’ll need to restructure. There are two ways to do this, but one will require a couple of new packages: tidyr and tidyverse.\n\nlibrary(tidyverse)\nlibrary(tidyr)\n\nOne way to restructure:\n\nathletes2 &lt;- athletes %&gt;% pivot_longer(cols=c('Baseball','Soccer'), names_to='Sport',values_to='Weight')\n\nathletes2\n\n# A tibble: 32 × 2\n   Sport    Weight\n   &lt;chr&gt;     &lt;int&gt;\n 1 Baseball    190\n 2 Soccer      165\n 3 Baseball    200\n 4 Soccer      190\n 5 Baseball    187\n 6 Soccer      185\n 7 Baseball    182\n 8 Soccer      187\n 9 Baseball    192\n10 Soccer      183\n# ℹ 22 more rows\n\n\nAnother way:\n\nathletes3&lt;-cbind(stack(athletes[1:2]))\n\nathletes3\n\n   values      ind\n1     190 Baseball\n2     200 Baseball\n3     187 Baseball\n4     182 Baseball\n5     192 Baseball\n6     205 Baseball\n7     185 Baseball\n8     177 Baseball\n9     186 Baseball\n10    210 Baseball\n11    198 Baseball\n12    180 Baseball\n13    182 Baseball\n14    193 Baseball\n15    200 Baseball\n16    195 Baseball\n17    165   Soccer\n18    190   Soccer\n19    185   Soccer\n20    187   Soccer\n21    183   Soccer\n22    189   Soccer\n23    170   Soccer\n24    182   Soccer\n25    156   Soccer\n26    168   Soccer\n27    173   Soccer\n28    158   Soccer\n29    150   Soccer\n30    172   Soccer\n31    180   Soccer\n32    184   Soccer\n\n#rename columns#\n#Do I need to do this? No. Does it help me keep track of which variable is which? Yes#\ncolnames(athletes3)[1]=\"weight\"\ncolnames(athletes3)[2]=\"sport\"\n\nathletes3\n\n   weight    sport\n1     190 Baseball\n2     200 Baseball\n3     187 Baseball\n4     182 Baseball\n5     192 Baseball\n6     205 Baseball\n7     185 Baseball\n8     177 Baseball\n9     186 Baseball\n10    210 Baseball\n11    198 Baseball\n12    180 Baseball\n13    182 Baseball\n14    193 Baseball\n15    200 Baseball\n16    195 Baseball\n17    165   Soccer\n18    190   Soccer\n19    185   Soccer\n20    187   Soccer\n21    183   Soccer\n22    189   Soccer\n23    170   Soccer\n24    182   Soccer\n25    156   Soccer\n26    168   Soccer\n27    173   Soccer\n28    158   Soccer\n29    150   Soccer\n30    172   Soccer\n31    180   Soccer\n32    184   Soccer\n\n\nNow we can get the dotplots.\n\ndotplot&lt;-ggplot(athletes2, aes(Weight)) + geom_dotplot(aes(color=Sport)) + facet_grid(~Sport)\ndotplot\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also get the confidence interval in R:\n\nt.test(Weight~Sport,data=athletes2)\n\n\n    Welch Two Sample t-test\n\ndata:  Weight by Sport\nt = 4.3061, df = 27.95, p-value = 0.0001846\nalternative hypothesis: true difference in means between group Baseball and group Soccer is not equal to 0\n95 percent confidence interval:\n  8.846974 24.903026\nsample estimates:\nmean in group Baseball   mean in group Soccer \n               191.375                174.500 \n\n\nBy default, R calculates 95% confidence intervals. We can change this with the conf.level= statement:\n\nt.test(Weight~Sport,data=athletes2,conf.level=0.90)\n\n\n    Welch Two Sample t-test\n\ndata:  Weight by Sport\nt = 4.3061, df = 27.95, p-value = 0.0001846\nalternative hypothesis: true difference in means between group Baseball and group Soccer is not equal to 0\n90 percent confidence interval:\n 10.20813 23.54187\nsample estimates:\nmean in group Baseball   mean in group Soccer \n               191.375                174.500 \n\n\nFor practice, find a research question you can answer using variables in the sleep study data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference for Means (Chapters 19-22)</span>"
    ]
  },
  {
    "objectID": "Section-3-Means.html#inference-for-comparing-paired-means-chapter-21",
    "href": "Section-3-Means.html#inference-for-comparing-paired-means-chapter-21",
    "title": "3  Inference for Means (Chapters 19-22)",
    "section": "3.3 Inference for Comparing Paired Means (Chapter 21)",
    "text": "3.3 Inference for Comparing Paired Means (Chapter 21)\nEverything we’ve done so far has assumed independence among observations. If we only had one group, it was just independence among observations. If we had two or more groups, it was independence between and within groups. Now, we’ll turn our attention to a common situation: dependence between groups. Specifically, a particular dependency–pairing. This occurs in before/after studies, other studies in which subjects are matched. For example, considering the price of a item purchased from two different retailers.\nIn these situations, we generally take the difference between the two values, and consider the difference as our observation. So, for example, if we want to compare cost of textbooks between the campus bookstore and Amazon, we’d randomly select a set of book titles, and find their price at both the bookstore and Amazon. We’d find the difference in price, and use those differences as our observations.\nNote that we’re distinguishing between difference in means (Chapter 20) and mean difference (Chapter 21).\n\nParameters:\n\n\n\n\nObserved Statistics:\n\n\n\nGood news: we’ve already seen how to construct mathematical model tests and confidence intervals here! We just use the same techniques we used for a single mean (Chapter 19), but on the differences.\nHowever, randomization tests didn’t really work with one mean in Chapter 19 because there was nothing to randomize. They will work here!\n\n3.3.1 Randomization test for mean difference (matched pairs)\nExample: Suppose you are playing baseball and hit a hard line drive. You want to turn a single into a double. Does the path you take to round first base make a difference? A masters thesis way back in 1970 considered the difference between a “narrow angle” and a “wide angle” around first base. Suppose we have 22 baseball players who have volunteered to participate. There are a couple ways we could design an experiment to see if there is a difference.\n\nRandomly assign 11 players to run a wide angle and 11 players to run a narrow angle. Problems: some players may be faster than others. Ideally, randomization will equally distribute the speedy runners between the two groups, but there is no guarantee. Speed could be a confounding variable.\nHave each of the 22 runners run both angles, with the angle run first randomized using a coin. This allows each player to serve as their own control.\n\nThe second option is what the thesis writer did–he randomly determined the angle the player would take first. He then used a stopwatch the time the run from going from a spot 35 feet past home to a spot 15 feet before 2nd base. After a rest period, the runner then ran the second angle. This controls for runner-to-runner variability. It’s important to randomize the order of the treatments, where possible! (This isn’t possible in before-and-after type studies.)\nLet’s look at the data:\n\n\n\n\n\n\n\n\nParameter of interest:\n\n\nHypotheses of interest:\n\n\nObserved statistic:\n\n\nLike before, we’re trying to determine if it’s surprising to see such a large difference as \\(\\bar x_d = 0.075\\) just by chance, if running strategy has no effect on running time.\nHere’s how the randomization test works: if running strategy really doesn’t make a difference, then the two times for each runner were going to be the same two times regardless of which strategy was used. Any difference was just by chance, perhaps which one they ran first. That is, it really doesn’t matter which value we call wide angle time and which value we call narrow angle time–the two times are completely interchangeable or swappable. This idea of swapping is how we’ll do the randomization.\nIn the two sample randomization test, the explanatory variable was randomly assigned to the response. We shuffled all the cards, and randomly dealt them into the two stacks. Here, randomization occurs within an observational unit (in our example, a baseball player). So, the two times will stay assigned to the same player, but we’ll randomly decide which time is narrow and which is wide using a coin flip. If the coin comes up, we swap the times. If the coin comes up tails, we don’t swap.\nWe’re going to do these in the applet, because I think it’s easiest to see the swapping. Let’s try it:\n\nGo to applet, do one randomization. In our randomization, how many players had their times swapped? The mean difference from this first randomization is shown in the applet. Like other randomization tests, we’ll need to do this over and over. We’ve built up an estimate of the sampling distribution for the mean difference.\nNow, just like before, we’ll see how unusual our observed \\(\\bar x_d = 0.075\\) is. Remember this is a two-sided test. None of our randomizations resulted in a mean difference more extreme than \\(\\bar x_d =0.075\\). So, our p-value is approximately 0, and it looks like we do have evidence that base-running strategy has an impact on running time. We can reject the null hypothesis, and conclude that there is a difference in the strategies.\n\n\n\n3.3.2 Bootstrap confidence intervals for mean difference (matched pairs)\nThe bootstrap approach to finding a confidence interval for \\(\\mu_d\\) is almost identical to the method for finding a bootstrap confidence interval for a single mean. The difference is in the interpretation.\n\nTake bootstrap samples from the observed differences\nLet’s look at the R code\n\n\nbases&lt;-read.csv(\"bases.csv\",header=TRUE)\nbases\n\n   id narrow wide\n1   1   5.50 5.55\n2   2   5.70 5.75\n3   3   5.60 5.50\n4   4   5.50 5.40\n5   5   5.85 5.70\n6   6   5.55 5.60\n7   7   5.40 5.35\n8   8   5.50 5.35\n9   9   5.15 5.00\n10 10   5.80 5.70\n11 11   5.20 5.10\n12 12   5.55 5.45\n13 13   5.35 5.45\n14 14   5.00 4.95\n15 15   5.50 5.40\n16 16   5.55 5.50\n17 17   5.55 5.35\n18 18   5.50 5.55\n19 19   5.45 5.25\n20 20   5.60 5.40\n21 21   5.65 5.55\n22 22   6.30 6.25\n\nbases$diff&lt;-bases$narrow-bases$wide\n\n##Bootstrap confidence intervals##\n#Set up an empty data set with 2 columns: simulation number, bootstrap mean#\n\nboot.samples&lt;-data.frame(sim=1:1000,mean_diff=NA)\n\n#For each row in the data set, draw a bootstrap sample from the original data and find#\n# mean_diff#\n\nfor(i in 1:1000){\n  boot.samples$mean_diff[i]&lt;-mean(sample(bases$diff,size=22,replace=TRUE))\n}\n\n#Histogram#\nboot.hist&lt;-ggplot(boot.samples, aes(mean_diff)) + geom_histogram(binwidth=0.005)\n\n#See the plot#\nboot.hist\n\n\n\n\n\n\n\n#To get the bootstrap percentile confidence interval, #\n#start by ranking the bootstrap means from smallest to largest #\nrankmean&lt;-sort(boot.samples$mean_diff)\n\n#Lower endpoint is the 2.5th percentile (95% confidence)#\nlower&lt;-rankmean[25]\nlower\n\n[1] 0.03863636\n\n#Upper endpoint is the 97.5th percentile (95% confidence)#\nupper&lt;-rankmean[975]\nupper\n\n[1] 0.1090909\n\n#Bootstrap standard error of the mean#\nsd(rankmean)\n\n[1] 0.01844355\n\n\n\nBootstrap percentile confidence interval:\n\n\n\nBootstrap SE confidence interval:\n\n\n\n\nWhat would happen if we (incorrectly) ignored the pairing? Let’s find a 95% confidence interval, assuming the two samples are independent.\n\n##INCORRECT ANALYSIS##\n#Set up an empty data set with 4 columns: simulation number, bootstrap mean for narrow, bootstrap mean for wide, difference#\n\nboot.samples&lt;-data.frame(sim=1:1000,mean_narrow=NA,mean_wide=NA,diff=NA)\n\n#For each row in the data set, draw a bootstrap sample from the original data and find#\n# mean_narrow and mean_wide#\n\nfor(i in 1:1000){\n  boot.samples$mean_narrow[i]&lt;-mean(sample(bases$narrow,size=22,replace=TRUE))\n  boot.samples$mean_wide[i]&lt;-mean(sample(bases$wide,size=22,replace=TRUE))\n  boot.samples$diff[i]&lt;-boot.samples$mean_narrow[i]-boot.samples$mean_wide[i]\n}\n\n#Histogram#\nboot.hist&lt;-ggplot(boot.samples, aes(diff)) + geom_histogram(binwidth=0.05)\n\n#See the plot#\nboot.hist\n\n\n\n\n\n\n\n#To get the bootstrap percentile confidence interval, #\n#start by ranking the bootstrap means from smallest to largest #\nrankmean&lt;-sort(boot.samples$diff)\n\n#Lower endpoint is the 2.5th percentile (95% confidence)#\nlower&lt;-rankmean[25]\nlower\n\n[1] -0.075\n\n#Upper endpoint is the 97.5th percentile (95% confidence)#\nupper&lt;-rankmean[975]\nupper\n\n[1] 0.2272727\n\n\n\n\n\nThe hardest part is determining whether we are dealing with independent samples or matched pairs. Let’s talk through 21.2, 21.3, 21.4, and 21.5\n\n\n\n3.3.3 Mathematical model approach for mean difference (matched pairs)\nThe mathematical model approach to matched pairs is the same as the one sample analysis, but carried out on differences. The changes come in the form of the hypotheses and interpretation of the confidence interval.\nWe still need to check conditions!\n\nIndependence: among observations (we know the observations within an observation are not independent)\nLarge enough sample size: no extreme outliers or strong skew\n\nExample: A study carried out by Cai et al. (2019) aimed to determine whether laugh tracks make dad jokes seem funnier. The researchers had a professional comedian record 40 dad jokes. They had people listen to the jokes and rate how funny they were on a 7 point scale, with 1 being not funny at all and 7 being extremely funny. Other people listened to the same 40 jokes, but this time the researchers added a laugh track to the recording. The volunteers were randomized to either no laugh track or laugh track.\n\nWhy is this a paired scenario?\n\n\n\n\n\nWhat is the parameter?\n\n\n\n\nWhat are the hypotheses?\n\n\n\nHere are the observed statistics:\n\n\n\nLaugh Track?\n\\(n\\)\nSample mean\nSample SD\n\n\n\n\nWith\n40\n3.010\n0.490\n\n\nWithout\n40\n2.715\n0.507\n\n\nDiff=with-without\n40\n\\(\\bar x_d=0.295\\)\n\\(s_d = 0.427\\)\n\n\n\nHypothesis test:\n\n\n\n\n\n\n90% confidence interval:\n\n\n\n\n\n\nWe can also do this in R, adding paired=TRUE to the t.test code. Let’s try it with the base running data.\n\nt.test(bases$narrow,bases$wide,paired=TRUE)\n\n\n    Paired t-test\n\ndata:  bases$narrow and bases$wide\nt = 3.9837, df = 21, p-value = 0.0006754\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.03584814 0.11415186\nsample estimates:\nmean difference \n          0.075",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference for Means (Chapters 19-22)</span>"
    ]
  },
  {
    "objectID": "Section-3-Means.html#inference-for-comparing-many-means-chapter-22",
    "href": "Section-3-Means.html#inference-for-comparing-many-means-chapter-22",
    "title": "3  Inference for Means (Chapters 19-22)",
    "section": "3.4 Inference for Comparing Many Means (Chapter 22)",
    "text": "3.4 Inference for Comparing Many Means (Chapter 22)\nWe’re going to start this section by considering an example. The data are in the file ‘mice.csv.’\nExample: These data come from an experiment to determine if exercise confers some resilience to stress. Mice were randomly assigned to either an enriched environment (exercise wheel) or standard environment, and spent three weeks there. After that time, they were exposed for five minutes per day for two weeks to a “mouse bully”–a mouse very strong, aggressive, and territorial. After those two weeks, anxiety in the mice was measured, as amount of time hiding in a dark compartment. Mice that are more anxious spend more time in darkness. We want to determine if there is a difference in time spent in darkness for the two groups of mice.\n\nmice&lt;-read.csv(\"mice.csv\",header=TRUE)\nhead(mice)\n\n    Envr Time\n1 Enrich  259\n2 Enrich  280\n3 Enrich  138\n4 Enrich  227\n5 Enrich  203\n6 Enrich  184\n\n\nWe already know how to answer this research question!\n\n\n\n\n\n\n\n\n\nLet’s first plot the data\n\n\n\n\n\n\n\n\n\nIt definitely looks like there’s a difference between the groups! We can find the group means and standard deviations. We’ll also add the sample means to the plot.\n\naggregate(mice$Time, by=list(mice$Envr), FUN=mean)\n\n  Group.1        x\n1  Enrich 217.4286\n2     Std 438.7143\n\naggregate(mice$Time, by=list(mice$Envr), FUN=sd)\n\n  Group.1        x\n1  Enrich 47.52844\n2     Std 37.68162\n\n\n\n\n\n\n\n\n\n\n\nWe’re testing H\\(_0: \\mu_1 = \\mu_2\\), and assume this is true to construct the test. The overall common sample mean is \\(\\bar x = 328.07\\).\n\nt.test(Time~Envr,data=mice)\n\n\n    Welch Two Sample t-test\n\ndata:  Time by Envr\nt = -9.6526, df = 11.407, p-value = 7.885e-07\nalternative hypothesis: true difference in means between group Enrich and group Std is not equal to 0\n95 percent confidence interval:\n -271.5245 -171.0470\nsample estimates:\nmean in group Enrich    mean in group Std \n            217.4286             438.7143 \n\n\n\n\n\n\n\n\n\n\n\nIt turns out the difference between the two groups will also manifest itself in the variances. There will be variation between the group means and the overall mean, as well as variation between the data points and their group means.\nRemember how sample variance is calculated:\n\n\n\n\n\n\nWe’re exploring how far, on average, observations are from the mean (squared). So, variance has to be positive. If there is a difference between the group means, the first kind of variation (between the group means and the overall mean) will be much greater than the second kind of variance (between the data points and their group mean). We can test whether the first variance is bigger than the second using an \\(F\\) statistic, just like we did in the last section when we were comparing two variances:\n\\[\nF = \\frac{\\hbox{variance between group means and overall mean}}{\\hbox{variance between the data points and their group mean}}\n\\]\nIf the variances are about equal, there’s no evidence of a difference between the group means–they vary as much from the overall mean as data points vary from their group mean. This will result in an \\(F\\) statistic of about 1. If there is a difference between the group means, the first kind of variation (between the group means and the overall mean) will be much greater than the second kind of variance (between the data points and their group mean). This will result in an \\(F\\) statistic greater than 1.\n\nFor the mice data:\n\n\n\n\n\nNotice!\n\n\n\n\n\nWe made some assumptions to carry out the \\(t\\)-test:\n\napproximate normality (no extreme outliers, no strong skew)\nindependence between groups and between observations\nconstant variance (we didn’t make a big deal of this one, but mentioned it)\n\nWe can summarize these assumptions very succinctly, and to do so we’re going to introduce some new notation.\nConsider a random sample of observations from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). If we let \\(Y_1, Y_2,\\dots, Y_n\\) represent our data points we can summarize this as:\n\n\n\n\nOr another way:\n\n\n\n\n\n\n\n\n\nThis is a statistical model with 2 parameters: \\(\\mu\\) and \\(\\sigma^2\\).\n\n\n\n\nIf we have two samples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we have more than two samples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s start with some summary statistics \\[\\begin{eqnarray*}\n    Y_{i\\cdot} &=& \\sum_{j=1}^{n_i} Y_{ij} = i^{th} \\hbox{ sample total} \\\\\n    \\bar Y_{i\\cdot} &=& \\frac{1}{n_i} \\sum_{j=1}^{n_i} Y_{ij} = i^{th} \\hbox{ sample mean} \\\\\n    Y_{\\cdot \\cdot} &=& \\sum_{i=1}^{t} \\sum_{j=1}^{n_i} Y_{ij} = \\hbox{ grand total} \\\\\n    \\bar Y_{\\cdot \\cdot} &=& \\frac{1}{N} \\sum_{i=1}^{t} \\sum_{j=1}^{n_i} Y_{ij} = \\hbox{ grand mean  } (N=\\sum_{i=1}^{n_i} n_i)\n\\end{eqnarray*}\\]\n\nExample: A student carried out an experiment to investigate handwashing methods: water only, regular soap, antibacterial soap, and alcohol spray. Each treatment was replicated 8 times, and bacteria count was observed. The data are in ‘handwash.csv’.\n\n\n\n\n\n\n\n\n\n\n\n  Group.1     x\n1  ABSoap  92.5\n2 Alcohol  37.5\n3    Soap 106.0\n4   Water 117.0\n\n\n  Group.1        x\n1  ABSoap 41.96257\n2 Alcohol 26.55991\n3    Soap 46.95895\n4   Water 31.13106\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember how to calculate the sample variance, \\(S^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar y)^2}{n-1}\\). We’re going to look at three difference variances. Let’s assume for simplicity that \\(n_i = n\\) (all groups have equal sample size, this is not really necessary, it’s just to make it easier to look at notation):\n\nTotal Variance. Another name for the numerator is total sum of squares.\n\n\n\n\n\n\n\n\nError (Within-Group) Variance. Another name for the numerator is the error sum of squares.\n\n\n\n\n\n\n\n\n\n\n\n\nModel (Between-Group) Variance. Another name for the numerator is the treatment (model) sum of squares.\n\n\n\n\n\n\nTo see what this is measuring, first consider the ‘inside’ sum:\n\n\n\n\n\nThis is still an estimate of variance, but it’s an estimate of \\(\\sigma^2/n\\), because these are means. In order to be able to compare fairly to the error variance we must multiply by \\(n\\) (only works with equal sample sizes) or, equivalently, take the sum from \\(j=1\\) to \\(n\\):\n\nWe can’t lose sight of what we’re interested in here: testing H\\(_0: \\mu_1 = \\mu_2\\). If H\\(_0\\) is true, \\(\\bar y_1\\) and \\(\\bar y_2\\) should not be different from \\(\\bar y_{\\cdot \\cdot}\\). This means that error variance should be about equal to model variance (both would estimate \\(\\sigma^2\\)). If H\\(_0\\) is not true, model variance will be larger because of the deviations of the group averages from the grand average. If it’s much larger, this gives us evidence against H\\(_0\\).\nWhy do we worry about three variances when we only use two (error and model) to get the \\(F\\) stat? It turn out that: \\[\n\\hbox{Total SS } = \\hbox{Model SS } + \\hbox{ Error SS}\n\\] For the mice data:\n\\[\\begin{eqnarray*}\n        \\hbox{Total SS } &=& (259-328.07)^2 + \\cdots + (231-328.07)^2 + (394-328.07)^2 + \\cdots + (454-328.07)^2 = 193459 \\\\\n        \\hbox{Error SS } &=& (259-217.43)^2 + \\cdots + (231-217.43)^2 + (394-438.71)^2 + \\cdots + (454-438.71)^2 = 22073 \\\\\n        \\hbox{Model SS } &=& 6(217.43-328.07)^2 + 6(438.71-328.07)^2 =  171386 \\\\\n\\end{eqnarray*}\\]\nTo convert these sums of squares into variances (which we call mean squares), they must be divided by denominators noted above. These are degrees of freedom, and have the same relationship as the sums of squares do: \\[\n\\hbox{Total } df = \\hbox{ Model } df + \\hbox{ Error } df\n\\]\nIn our mice example, we have\n\\[\n\\hbox{Total } df = \\hbox{ Model } df + \\hbox{ Error } df\n\\]\n\n\n\n\nWe often summarize our calculations in a table (\\(df\\) assuming equal sample sizes):\n\n\n\nSource\n\\(df\\)\nSS\nMS\n\n\n\n\nModel\n\\(t-1\\)\nSSModel\nMSModel\n\n\nError\n\\(t(n-1)\\)\nSSError\nMSError\n\n\nTotal\n\\(nt-1\\)\nSSTotal\n\n\n\n\n\nThe MSError (usually called MSE) is our estimate of \\(\\sigma^2\\). In our mice example, we get the table:\n\n\n\nSource\n\\(df\\)\nSS\nMS\n\n\n\n\nModel\n1\n171386\n171386\n\n\nError\n12\n22073\n1839\n\n\nTotal\n13\n193459\n\n\n\n\nTo test H\\(_0: \\mu_1 = \\mu_2\\) we use the F stat: \\[\nF = \\frac{\\hbox{MSModel}}{\\hbox{MSError}} = \\frac{171386}{1839} = 93.2\n\\] and we can add this to the table:\n\n\n\nSource\n\\(df\\)\nSS\nMS\nF\n\n\n\n\nModel\n1\n171386\n171386\n93.2\n\n\nError\n12\n22073\n1839\n\n\n\nTotal\n13\n193459\n\n\n\n\n\nWhat we’ve just done is called an Analysis of Variance (ANOVA), and the resulting table is called an ANOVA table. It’s a single hypothesis test to check whether the means across many groups are equal. Specifically, it’s testing:\n\n\n\n\n\nWe still have assumptions: - Independence between and among groups - Responses/errors are approximately normal - Variability across groups is about equal\nWe still don’t know if 93.2 is enough greater than 1 to determine there’s a difference! We have two options:\n\nRandomization test: Like for two means, write all responses on cards. Shuffle, and deal into as many stacks as there are groups with stack size corresponding to group size. Find \\(F\\) for the shuffle. Repeat many times, and see how unusual our observed \\(F\\) statistic is. We can do this in the applet or in R.\nMathematical model: \\(F\\) Test\nAssuming H\\(_0\\) is true and the assumptions are met, \\(F\\) follows and \\(F\\)-distribution with \\(df_1 = t-1\\) and \\(df_2 = N-t\\) (\\(N\\) is the total number of observations). We can use pf() in R to find p-values\n\n\n\npf(93.2,df1=1,df2=12,lower.tail=FALSE)\n\n[1] 5.232224e-07\n\n\nThe p-value typically gets added to the table as well:\n\n\n\nSource\n\\(df\\)\nSS\nMS\nF\np-value\n\n\n\n\nModel\n1\n171386\n171386\n93.2\n0.0000005\n\n\nError\n12\n22073\n1839\n\n\n\n\nTotal\n13\n193459\n\n\n\n\n\n\nThis is the only time we’ll do an ANOVA by hand! Let’s do the same in R.\n\nanova(lm(Time~Envr, data=mice))\n\nAnalysis of Variance Table\n\nResponse: Time\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nEnvr       1 171386  171386  93.173 5.24e-07 ***\nResiduals 12  22073    1839                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nExample: Let’s now carry out the ANOVA on the handwashing data. We’ll start by writing the model and sketching the ANOVA table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nanova(lm(Bacteria~Method,data=handwash))\n\nAnalysis of Variance Table\n\nResponse: Bacteria\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nMethod     3  29882  9960.7  7.0636 0.001111 **\nResiduals 28  39484  1410.1                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe’ll do some more examples, focusing on sketching the ANOVA table.\nExample: A teacher takes a random sample of 30 student GPAs, along with where they chose to sit in a classroom (front, middle, back). We want to see if mean GPA differs based on where a student sits.\nExample: Baseball run time. The data gives run time in seconds for 50 yards for 29 players at three different positions (OF, IF, C).\n A group of college students wanted to see whether there was an association between students’ major and the time (in seconds) to complete a small paper-and-pencil puzzle. They took a random sample of 40 students, and they grouped majors into four categories: applied science (as), natural science (ns), social science (ss), and arts/humanities (ah).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Inference for Means (Chapters 19-22)</span>"
    ]
  }
]